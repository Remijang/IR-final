11.4 Matrices and Linear Equations

We are moving from geometry to algebra. Eventually we get back to calculus, where functions are nonlinear—but linear equations come first. In Chapter 1,  produced a line. Two equations produce two lines. If they cross, the intersection point solves both equations—and we want to find it.

Three equations in three variables  produce three planes. Again they go through one point (usually). Again the problem is to find that intersection point —which solves the three equations.

The ultimate problem is to solve  equations in  unknowns. There are  hyperplanes in  -dimensional space, which meet at the solution. We need a test to be sure they meet. We also want the solution. These are the objectives of linear algebra, which joins with calculus at the center of pure and applied mathematics.

Like every subject, linear algebra requires a good notation. To state the equations and solve them, we introduce a “matrix.” The problem will be  : The solution will be  . It remains to understand where the equations come from, where the answer comes from, and what the matrices  and  stand for.

TWO EQUATIONS IN TWO UNKNOWNS

Linear algebra has no reason to choose one variable as special. The equation   separates  from  : A better equation for a line is  : (A vertical line like  appears when  : The first form did not allow slope  :) This section studies two lines:

By solving both equations at once, we are asking  to lie on both lines. The practical question is: Where do the lines cross ? The mathematician’s question is: Does a solution exist and is it unique ?

To understand everything is not possible. There are parts of life where you never know what is going on (until too late). But two equations in two unknowns can have no mysteries. There are three ways to write the system—by rows, by columns, and by matrices. Please look at all three, since setting up a problem is generally harder and more important than solving it. After that comes the concession to the real world: we compute  and  :

EXAMPLE 1 How do you invest  5000\ a year interest, if a money market account pays  and a deposit account pays  ?

Set up equations by rows: With  dollars at  the interest is .  : With  dollars at  the interest is .  : One row for principal, another row for interest:

11.4 Matrices and Linear Equations

Same equations by columns: The left side of (2) contains  times one vector plus  times another vector. The right side is a third vector. The equation by columns is

Same equations by matrices: Look again at the left side. There are two unknowns  and  ; which go into a vector  : They are multiplied by the four numbers 1; .05; 1; and .10; which go into a two by two matrix  : The left side becomes  matrix times  vector:

Now you see where the “rows” and “columns” came from. They are the rows and columns of a matrix. The rows entered the separate equations (2). The columns entered the vector equation (3). The matrix-vector multiplication  is defined so that all these equations are the same:

 is the coefficient matrix. The unknown vector is  : The known vector on the right side, with components 5000 and 400; is  : The matrix equation is  :

This notation  continues to apply when there are more equations and more unknowns. The matrix  has a row for each equation (usually m rows). It has a column for each unknown (usually  columns). For 2 equations in 3 unknowns it is a 2 by 3 matrix (therefore rectangular). For 6 equations in 6 unknowns the matrix is 6 by 6 (therefore square). The best way to get familiar with matrices is to work with them. Note also the pronunciation: “matrisees” and never “matrixes.”

Answer to the practical question The solution is  ;  : That is the intersection point in the row picture (Figure 11.16). It is also the correct combination in the column picture. The matrix equation checks both at once, because matrices are multiplied by rows or by columns. The product either way is  :

Singular case In the row picture, the lines cross at the solution. But there is a case that gives trouble. When the lines are parallel, they never cross and there is no solution. When the lines are the same, there is an infinity of solutions:

This trouble also appears in the column picture. The columns are vectors a and b: The equation  is the same as  : We are asked to find the combination of a and  (with coefficients  and  ) that produces  : In the singular case a and  lie along the same line (Figure 11.17). No combination can produce  ; unless it happens to lie on this line.

The investment problem is nonsingular, and  equals  : We also drew

EXAMPLE 2 : The matrix  multiplies  to solve  and  0:

The crossing point is  in the row picture. The solution is  in the column picture (Figure 11.17b). Then 1 times a plus 1 times  equals the right side  :

SOLUTION BY DETERMINANTS

Up to now we just wrote down the answer. The real problem is to find  and  when they are unknown. We solve two equations with letters not numbers:

The key is to eliminate  : Multiply the first equation by  and the second equation by  : Subtract the first from the second and the  ’s disappear:

To eliminate  ; subtract  ; times the second equation from  times the first:

What you see in those parentheses are 2 by 2 determinants! Remember from Section 11:3:

This number appears on the left side of (6) and (7). The right side of (7) is also a determinant—but it has  ’s in place of  ’s. The right side of (6) has  ’s in place of  ’s. So  and  are ratios of determinants, given by Cra mer’s Rule:

The investment example is solved by three determinants from the three columns:

Cramer’s Rule has  and  This is the solution. The singular case is when the determinant of A is zero—and we can’t divide by it.

11I Cramer’s Rule breaks down when det  —which is the singular case. Then the lines in the row picture are parallel, and one column is a multiple of the other column.

EXAMPLE 3 The lines  are parallel. The determinant is zero:

The lines in Figure 11.17a don’t meet. Notice the columns:  is a multiple of  :

One final comment on 2 by 2 systems. They are small enough so that all solution methods apply. Cramer’s Rule uses determinants. Larger systems use elimination (3 by 3 matrices are on the borderline). A third solution (the same solution!) comes from the inverse matrix  ; to be described next. But the inverse is more a symbol for the answerthan a new way of computing it, because to find  we still use determinants or elimination.

THE INVERSE OF A MATRIX

The symbol  is pronounced “A inverse.” It stands for a matrix—the one that solves  : I think of  as a matrix that takes  to  : Then  is a matrix that takes  back to  : If  then  (provided the inverse exists). This is exactly like functions and inverse functions:  and  : Our goal is to find  when we know  :

The first approach will be very direct. Cramer’s Rule gave formulas for  and  ; the components of  : From that rule we can read off  ; assuming that  is not zero.  is det  and we divide by it:

The matrix on the right (including  in all four entries) is  : Notice the sign pattern and the subscript pattern. The inverse exists if  is not zero—this is important. Then the solution comes from a matrix-vector multiplication,  times d: We repeat the rules for that multiplication:

DEFINITION A matrix  times a vector  equals a vector of dot products:

Equation (8) follows this rule with  and  : Look at Example 1:

There stands the inverse matrix. It multiplies  to give the solution  :

The formulas work perfectly, but you have to see a direct way to reach  : Multiply both sides of  by  . The multiplication “cancels”  on the left side, and leaves  : This approach comes next.

MATRIX MULTIPLICATION

To understand the power of matrices, we must multiply them. The product of  with  is a matrix times a vector. But that multiplication can be done another way. First  multiplies  ; a matrix times a matrix. The product  is another matrix (a very special matrix). Then this new matrix multiplies  :

The matrix-matrix rule comes directly from the matrix-vector rule. Effectively, a vector  is a matrix  with only one column. When there are more columns,  times  splits into separate matrix-vector multiplications, side by side:

DEFINITION A matrix  times a matrix  equals a matrix of dot products:

EXAMPLE 5 Multiplying  times  produces the “identity matrix” 

This identity matrix is denoted by  : It has 1’s on the diagonal and 0’s off the diagonal. It acts like the number 1: Every vector satisfies  :

The next section moves to three equations. The algebra gets more complicated (and 4 by 4 is worse). It is not easy to write out  : So we stay longer with the 2 by 2 formulas, where each step can be checked. Multiplying  by the inverse matrix gives  —and the left side is  :

EXAMPLE 6  rotates every  to  ; through the angle  :   
Question 1 Where is the vector    
Question 2 What is    
Question 3 Which vector  is rotated into    
Solution 1 v rotates into 

Solution 2 det  so 

Solution 3 If  the 

Historical note I was amazed to learn that it was Leibniz (again!) who proposed the notation we use for matrices. The entry in row  and column  is  : The identity matrix has  and  : This is in a linear algebra book by Charles Dodgson—better known to the world as Lewis Carroll, the author of Alice in Wonderland. I regret to say that he preferred his own notation  instead of  : “I have turned the symbol toward the left, to avoid all chance of confusion with  :” It drove his typesetter mad.

PROJECTION ONTO A PLANE  LEAST SQUARES FITTING BY A LINE

We close with a genuine application. It starts with three-dimensional vectors  and leads to a 2 by 2 system. One good feature:  can be  -dimensional with no change in the algebra. In practice that happens. Second good feature: There is a calculus problem in the background. The example is to fit points by a straight line.

There are three ways to state the problem, and they look different:

1. Solve  as well as possible (three equations, two unknowns  and  ).   
2. Project the vector d onto the plane of the vectors a and b:   
3. Find the closest straight line (“least squares”) to three given points.

Figure 11.19 shows a three-dimensional vector  above the plane of a and b: Its projection onto the plane is  : The numbers  and  are unknown, and our goal is to find them. The calculation will use the dot product, which is always the key to right angles.

The difference  is the “error.” There has to be an error, because no combination of a and  can produce d exactly. (Otherwise  is in the plane.) The projection  is the closest point to  ; and it is governed by one fundamental law: The error is perpendicular to the plane. That makes the error perpendicular to both vectors a and  :

Rewrite those as two equations for the two unknown numbers  and  :

These are the famous normal equations in statistics, to compute  and  and  :

EXAMPLE 7 For  and  and  ; solve equation (14):

Notice the three equations that we are not solving (we can’t):  is

For  there is no solution;  is not in the plane of a and b: For  there is a solution,  and  : The vector  is in the plane. The error  is  : This error is perpendicular to the columns  and .1; 2; 3/; so it is perpendicular to their plane.

SAME EXAMPLE (written as a line-fitting problem) Fit the points  and  and  as closely as possible (“least squares”) by a straight line.

Two points determine a line. The example asks the line  to go through three points. That gives the three equations in (15), which can’t be solved with two unknowns. We have to settle for the closest line, drawn in Figure 11.19b. This line is computed again below, by calculus.

Notice that the closest line has heights 1; 3; 5 where the data points have heights  : Those are the numbers in  and  ! The heights 1; 3; 5 fit onto a line; the heights  do not. In the first figure,  is in the plane and  is not. Vectors in the plane lead to heights that lie on a line.

Notice another coincidence. The coefficients  and  give the projection  : They also give the closest line  : All numbers appear in both figures.

Remark Finding the closest line is a calculus problem: Minimize a sum of squares. The numbers  and  that minimize  give the least squares solution:

Those are the three errors in equation (15), squared and added. They are also the three errors in the straight line fit, between the line and the data points. The projection minimizes the error (by geometry), the normal equations (14) minimize the error (by algebra), and now calculus minimizes the error by setting the derivatives of  to zero.

The new feature is this:  depends on two variables  and  : Therefore  has two derivatives. They both have to be zero at the minimum. That gives two equations for  and  :

 derivative of  is zero:    derivative of  is zero:

When we divide by 2; those are the normal equations  and  22: The minimizing  and  from calculus are the same numbers  and 2:

The  derivative treats  as a constant. The  derivative treats  as a constant. These are partial derivatives. This calculus approach to least squares is in Chapter 13, as an important application of partial derivatives.

We now summarize the least squares problem—to find the closest line to  data points. In practice  may be 1000 instead of 3: The points have horizontal coordinates  : The vertical coordinates are  : These vectors  and  ; together with  ; determine a projection—the combination   that is closest to  : This problem is the same in  dimensions—the error  is perpendicular to aand  : That is still tested by dot products,  and   ; which give the normal equations for  and  :

11K The least squares problem projects  onto the plane of a and  : The projection is  ; in  dimensions. The closest line  ; in two dimensions. The normal equations (17) give the best  and  :