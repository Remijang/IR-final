[
    {
        "type": "text",
        "text": "11.2.5 The replace Operation ",
        "text_level": 1,
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "The task is here to find an entry in the set that can be replaced by a new entry. We do not wish to replace an element that is accessed very frequently. This will increase the number of cache misses. We ideally want to replace an element that has the least probability of being accessed in the future. However, it is difficult to predict future events. Hence, we need to make reasonable guesses based on past behavior. We can have different policies for the replacement of blocks in a cache. These are known as replacement schemes or replacement policies. ",
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "Definition 104 ",
        "text_level": 1,
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "A cache replacement scheme or replacement policy is a method to find an entry to evict in a cache set when space needs to be created to add a new entry. ",
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "Random Replacement Policy ",
        "text_level": 1,
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "The most trivial replacement policy is known as the random replacement policy. Here, we pick a block at random and replace it. This scheme is very simple to implement. However, it is not very optimal in terms of performance, because it does not take into account the behavior of the program and the nature of the memory access pattern. This scheme often ends up replacing very frequently accessed blocks. ",
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "FIFO Replacement Policy ",
        "text_level": 1,
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "The next scheme is slightly more complicated, and is known as the FIFO (first in first out) replacement policy. Here, the assumption is that the block that was brought into the cache at the earliest point of time, is the least likely to be accessed in the future. To implement the FIFO replacement policy, we need to add a counter to the tag array. Whenever, we bring in a block, we assign it a counter value equal to 0. We increment the counter values for the rest of the blocks. The larger is the counter, the earlier the block was brought into the cache. ",
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "Now to find a candidate for replacement, we need to find an entry with the largest value of the counter. This must be the earliest block. Unfortunately, the FIFO scheme does not strictly align with our principles of temporal locality. It penalizes blocks that are present in the cache for a long time. However, they may also be very frequently accessed blocks, and should not be evicted in the first place. ",
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "Let us now consider the practical aspects of implementing a FIFO replacement policy. The maximum size of the counter needs to be equal to the number of elements in a set, i.e., the associativity of the cache. For example, if the associativity of a cache is 8, we need to have a 3 bit counter. The entry that needs to be replaced should have the largest counter value. ",
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "Note that in this case, the process of bringing in a new value into the cache is rather expensive. We need to increment the counters of all the elements in the set except one. However, cache misses, are more infrequent as compared to cache hits. Hence, the overhead is not significant in practice, and this scheme can be implemented without large performance overheads. ",
        "page_idx": 530
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 531
    },
    {
        "type": "text",
        "text": "LRU Replacement Policy ",
        "text_level": 1,
        "page_idx": 531
    },
    {
        "type": "text",
        "text": "The LRU (least recently used) replacement policy is known to be as one of the most efficient schemes. The LRU scheme follows directly from the definition of stack distance. We ideally want to replace a block that has the lowest chance of being accessed in the future. According to the notion of stack distance, the probability of being accessed in the future is related to the probability of accesses in the recent past. If a processor has been accessing a block frequently in the last window of $n$ ( $n$ is not a very large number) accesses, then there is a high probability that the block will be accessed in the immediate future. However, if the last time that a block was accessed is long back in the past, then the chances are unlikely that it will be accessed soon. ",
        "page_idx": 531
    },
    {
        "type": "text",
        "text": "In the LRU replacement policy, we maintain the time that a block was last accessed. We choose the block that was last accessed at the earliest point of time as a candidate for replacement. In a hypothetical implementation of an LRU replacement policy, we maintain a timestamp for every block. Any time that a block is accessed, its timestamp is updated to match the current time. For finding an appropriate candidate for replacement, we need to find the entry with the smallest timestamp in a set. ",
        "page_idx": 531
    },
    {
        "type": "text",
        "text": "Let us now consider the implementation of an LRU scheme. The biggest issue is that we need to do additional work for every read and write access to the cache. There will be a significant performance impact because typically 1 in 3 instructions are memory accesses. Secondly, we need to dedicate bits to save a timestamp that is sufficiently large. Otherwise, we need to frequently reset the timestamps of every block in a set. This process will induce a further slowdown, and additional complexity in the cache controller. Implementing an LRU scheme that is as close to an ideal LRU implementation as possible, and that does not have significant overheads, is thus a difficult task. ",
        "page_idx": 531
    },
    {
        "type": "text",
        "text": "Hence, let us try to design LRU schemes that use small timestamps (typically 1-3 bits), and approximately follow the LRU policy. Such schemes are called pseudo-LRU schemes. Let us outline a simple method for implementing a basic pseudo-LRU scheme. Note that we can have many such approaches, and the reader is invited to try different approaches and test them on a cache simulator such as Dinero [Edler and Hill, 1999], or sim-cache [Austin et al., 2002]. Instead of trying to explicitly mark the least recently used element, let us try to mark the more recently used elements. The elements that are not marked will automatically get classified as the least recently used elements. ",
        "page_idx": 531
    },
    {
        "type": "text",
        "text": "Let us start out by associating a counter with each block in the tag array. Whenever, a block is accessed (read/write), we increment the counter. However, once the counter reaches the maximum value, we stop incrementing it further. For example, if we use a 2-bit counter, then we stop incrementing the counter beyond 3. Now, we need to do something more. Otherwise, the counter associated with every block will ultimately reach 3 and stay there. To solve this problem, we can periodically decrement the counters of every block in a set by 1, or we can even reset them to 0. Subsequently, some counters will start increasing again. This procedure will ensure that for most of the time, we can identify the least recently used blocks by taking a look at the value of counters. The block associated with the lowest value of the counter is one of the least recently used blocks, and most likely the block that has been used the least in the recent past. Note that this approach does involve some amount of activity per access. However, incrementing a small counter has little additional overhead. Secondly, it is not in the critical path in terms of timing. It can be done in parallel or sometime later also. Finding a candidate for replacement involves looking at all the counters in a set, and finding the block with the lowest value of the counter. After we replace the block with a new block, most processors typically set the counter of the new block to the largest possible value. This indicates to the cache controller, that the new block should have the least priority with respect to being a candidate for replacement. ",
        "page_idx": 531
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 532
    }
]