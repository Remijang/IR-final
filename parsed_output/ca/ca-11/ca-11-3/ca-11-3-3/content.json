[
    {
        "type": "text",
        "text": "11.3.3 Reduction of Hit Time and Miss Penalty ",
        "text_level": 1,
        "page_idx": 539
    },
    {
        "type": "text",
        "text": "Hit Time ",
        "text_level": 1,
        "page_idx": 539
    },
    {
        "type": "text",
        "text": "The average memory access time can also be reduced by reducing the hit time and the miss penalty. To reduce the hit time, we need to use small and simple caches. However, by doing so, we increase the miss rate also. ",
        "page_idx": 539
    },
    {
        "type": "text",
        "text": "Miss Penalty ",
        "text_level": 1,
        "page_idx": 539
    },
    {
        "type": "text",
        "text": "Let us now discuss ways to reduce the miss penalty. Note that the miss penalty at level $i$ , is equal to the memory latency of the memory system starting at level $( i + 1 )$ . The traditional methods for reducing hit time, and miss rate can always be used to reduce the miss penalty at a given level. However, we are looking at methods that are exclusively targeted towards reducing the miss penalty. Let us first look at write misses in the L1 cache. In this case the entire block has to be brought into the cache from the L2 cache. This takes time ( $> 1 0$ cycles), and secondly unless the write has completed, the pipeline cannot resume. Hence, processor designers use a small set associative cache known as a write buffer (as shown in Figure 11.16). The processor can write the value to the write buffer, and then resume, or alternatively, it can write to the write buffer only if there is a miss in the L1 cache (as we have assumed). Any subsequent read needs to check the write buffer along with accessing the L1 cache. This structure is typically very small and fast (4-8 entries). Once, the data arrives in the L1 cache, the corresponding entry can be removed from the write buffer. Note that if a free entry is not available in the write buffer, then the pipeline needs to stall. Secondly, before the write miss has been serviced from the lower levels of the cache, it is possible that there might be another write to the same address. This can be seamlessly handled by writing to the allocated entry for the given address in the write buffer. ",
        "page_idx": 539
    },
    {
        "type": "image",
        "img_path": "images/271161630d8400b7faf620a88ae06e6a851d9c58c7fafe8e209f6737f479b700.jpg",
        "img_caption": [
            "Figure 11.16: Write buffer "
        ],
        "img_footnote": [],
        "page_idx": 540
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 540
    },
    {
        "type": "text",
        "text": "Let us now take a look at read misses. Let us start out by observing that the processor is typically interested in only up to 4 bytes per memory access. The pipeline can resume if it is provided those crucial 4 bytes. However, the memory system needs to fill the entire block before the operation can complete. The size of a block is typically between 32-128 bytes. It is thus possible to introduce an optimization here, if the memory system is aware of the exact set of bytes that the processor requires. In this case, the memory system can first fetch the memory word (4 bytes) that is required. Subsequently, or in parallel it can fetch the rest of the block. This optimization is known as critical word first. Then, this data can be quickly sent to the pipeline such that it can resume its operation. This optimization is known as early restart. Implementing both of these optimizations increases the complexity of the memory system. However, critical word first and early restart are fairly effective in reducing the miss penalty. ",
        "page_idx": 540
    }
]