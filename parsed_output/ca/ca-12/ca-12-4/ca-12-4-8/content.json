[
    {
        "type": "text",
        "text": "12.4.8 Multithreaded Processors ",
        "text_level": 1,
        "page_idx": 610
    },
    {
        "type": "text",
        "text": "Let us now look at a different method for designing multiprocessors. Up till now we have maintained that we need to have physically separate pipelines for creating multiprocessors. We have looked at designs that assign a separate program counter to each pipeline. However, let us look at a different approach that runs a set of threads on the same pipeline. This approach is known as multithreading. Instead of running separate threads on separate pipelines, we run them on the same pipeline. Let us illustrate this concept by discussing the simplest variant of multi-threading known as coarse-grained multithreading. ",
        "page_idx": 610
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 611
    },
    {
        "type": "text",
        "text": "Definition 131 Multithreading is a design paradigm that proposes to run multiple threads on the same pipeline. A processor that implements multithreading is known as a multithreaded processor. ",
        "page_idx": 611
    },
    {
        "type": "text",
        "text": "Coarse-Grained Multithreading ",
        "text_level": 1,
        "page_idx": 611
    },
    {
        "type": "text",
        "text": "Let us assume that we wish to run four threads on a single pipeline. Recall that multiple threads belonging to the same process have their separate program counters, stacks, registers; yet, they have a common view of memory. All these four threads have their separate instruction streams, and it is necessary to provide an illusion that these four threads are running separately. Software should be oblivious of the fact that threads are running on a multithreaded processor. It should perceive that each thread has its dedicated CPU. Along with the traditional guarantees of coherence and consistency, we now need to provide an additional guarantee, which is that software should be oblivious to multithreading. ",
        "page_idx": 611
    },
    {
        "type": "text",
        "text": "Let us consider a simple scheme, as shown in Figure 12.17. ",
        "page_idx": 611
    },
    {
        "type": "image",
        "img_path": "images/70ef9f9777d0366b3379aa1a268686f4012352ae8d5efe0e2acbcc4b7494f6ab.jpg",
        "img_caption": [
            "Figure 12.17: Conceptual view of coarse grain multithreading "
        ],
        "img_footnote": [],
        "page_idx": 611
    },
    {
        "type": "text",
        "text": "Here, we run thread 1 for $n$ cycles, then we switch to thread 2 and run it for $n$ cycles, then we switch to thread 3, and so on. After executing thread 4 for $n$ cycles, we start executing thread 1 again. To execute a thread we need to load its state or context. Recall that we had a similar discussion with respect to loading and unloading the state of a program in Section 10.8. We had observed that the context of the program comprises the flags register, the program counter, and the set of registers. We had observed that it is not necessary to keep track of main memory because the memory regions of different processes do not overlap, and in the case of multiple threads, we explicitly want all the threads to share the same memory space. ",
        "page_idx": 611
    },
    {
        "type": "text",
        "text": "Instead of explicitly loading and unloading the context of a thread, we can adopt a simpler approach. We can save the context of a thread in the pipeline. For example, if we wish to support coarse-grained multithreading then we can have four separate flags registers, four program counters, and four separate register files (one per each thread). Additionally, we can have a dedicated register that contains the id of the currently running thread. For example, if we are running thread 2, then we use the context of thread 2, and if we are running thread 3, we use the context of thread 3. In this manner it is not possible for multiple threads to overwrite each other\u2019s state. ",
        "page_idx": 612
    },
    {
        "type": "text",
        "text": "Let us now look at some subtle issues. It is possible that we can have instructions belonging to multiple threads at the same point of time in the pipeline. This can happen when we are switching from one thread to the next. Let us add a thread id field to the instruction packet, and further ensure that the forwarding and interlock logic takes the id of the thread into account. We never forward values across threads. In this manner it is possible to execute four separate threads on a pipeline with a negligible overhead of switching between threads. We do not need to engage the exception handler to save and restore the context of threads, or invoke the operating system to schedule the execution of threads. ",
        "page_idx": 612
    },
    {
        "type": "text",
        "text": "Let us now look at coarse-grained multithreading in entirety. We execute $n$ threads in quick succession, and in round robin order. Furthermore, we have a mechanism to quickly switch between threads, and threads do not corrupt each other\u2019s state. However, we still do not execute four threads simultaneously. Then, what is the advantage of this scheme? ",
        "page_idx": 612
    },
    {
        "type": "text",
        "text": "Let us consider the case of memory intensive threads that have a lot of irregular accesses to memory. They will thus frequently have misses in the L2 cache, and their pipelines need to be stalled for 100-300 cycles till the values come back from memory. Out-of-order pipelines can hide some of this latency by executing some other instructions that are not dependent on the memory value. Nonetheless, it will also stall for a long time. However, at this point, if we can switch to another thread, then it might have some useful work to do. If that thread suffers from misses in the L2 cache also, then we can switch to another thread and finish some of its work. In this way, we can maximize the throughput of the entire system as a whole. We can envision two possible schemes. We can either switch periodically every $n$ cycles, or switch to another thread upon an event such as an L2 cache miss. Secondly, we need not switch to a thread if it is waiting on a high latency event such as an L2 cache miss. We need to switch to a thread that has a pool of ready-to-execute instructions. It is possible to design numerous heuristics for optimizing the performance of a coarse-grained multithreaded machine. ",
        "page_idx": 612
    },
    {
        "type": "text",
        "text": "Important Point 20 ",
        "text_level": 1,
        "page_idx": 612
    },
    {
        "type": "text",
        "text": "Let us differentiate between software threads and hardware threads. A software thread is a subprogram that shares a part of its address space with other software threads. The threads can communicate with each other to co-operatively to achieve a common objective. In comparison, a hardware thread is defined as the instance of a software thread or a single threaded program running on a pipeline along with its execution state. A multithreaded processor supports multiple hardware threads on the same processor by splitting its resources across the threads. A software thread might physically be mapped to a separate processor, or to a hardware thread. It is agnostic to the entity that is used to execute it. The important point to be noted here is that a software thread is a programming language concept, whereas a hardware thread is physically associated with resources in a pipeline. We shall use the word \u201cthread\u201d for both software and hardware threads. The correct usage needs to be inferred from the context. ",
        "page_idx": 612
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 613
    },
    {
        "type": "text",
        "text": "Fine-Grained Multithreading ",
        "text_level": 1,
        "page_idx": 613
    },
    {
        "type": "text",
        "text": "Fine-grained multithreading is a special case of coarse-grained multithreading where the switching interval, $n$ , is a very small value. It is typically 1 or 2 cycles. This means that we quickly switch between threads. We can leverage grained multithreading to execute threads that are memory intensive. However, fine-grained multithreading is also useful for executing a set of threads that for example have long arithmetic operations such as division. In a typical processor, division operations and other specialized operations such as trigonometric or transcendental operations are slow (3-10 cycles). During this period when the original thread is waiting for the operation to finish, we can switch to another thread and execute some of its instructions in the pipeline stages that are otherwise unused. We can thus leverage the ability to switch between threads very quickly for reducing the idle time in scientific programs that have a lot of mathematical operations. ",
        "page_idx": 613
    },
    {
        "type": "text",
        "text": "We can thus visualize fine-grained multithreading to be a more flexible form of coarsegrained multithreading where we can quickly switch between threads and utilize idle stages to perform useful work. Note that this concept is not as simple as it sounds. The devil is in the details. We need elaborate support for multithreading in all the structures in a regular in-order or out-of-order pipeline. We need to manage the context of each thread very carefully, and ensure that we do not omit instructions, and errors are not introduced. A thorough discussion on the implementation of multithreading is beyond the scope of this book. ",
        "page_idx": 613
    },
    {
        "type": "text",
        "text": "The reader needs to appreciate that the logic for switching between threads is non-trivial. Most of the time the logic to switch between threads is a combination of time-based criteria (number of cycles), and event-based criteria (high latency event such as L2 cache miss or page fault). The heuristics have to be finely adjusted to ensure that the multithreaded processor performs well for a host of benchmarks. ",
        "page_idx": 613
    },
    {
        "type": "text",
        "text": "Simultaneous Multithreading ",
        "text_level": 1,
        "page_idx": 613
    },
    {
        "type": "text",
        "text": "For a single issue pipeline, if we can ensure that every stage is kept busy by using sophisticated logic for switching between threads, then we can achieve high efficiency. Recall that any stage in a single issue pipeline can process only one instruction per cycle. In comparison, a multiple issue pipeline can process multiple instructions per cycle. We had looked at multiple issue pipelines (both in-order and out-of-order) in Section 10.11. Moreover, we had defined the number of issue slots to be equal to the number of instructions that can be processed by the pipeline every cycle. For example, a 3 issue processor, can at the most fetch, decode, and finally execute 3 instructions per cycle. ",
        "page_idx": 613
    },
    {
        "type": "text",
        "text": "For implementing multithreading in multiple issue pipelines, we need to consider the nature of dependencies between instructions in a thread also. It is possible that fine and coarse-grained schemes do not perform well because a thread cannot issue instructions to the functional units for all the issue slots. Such threads are said to have low instruction level parallelism. If we use a 4 issue pipeline, and the maximum IPC for each of our threads is 1 because of dependencies in the program, then 3 of our issue slots will remain idle in each cycle. Thus, the overall IPC of our system of 4 threads will be 1, and the benefits of multithreading will be limited. ",
        "page_idx": 613
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 614
    },
    {
        "type": "text",
        "text": "Hence, it is necessary to utilize additional issue slots such that we can increase the IPC of the system as a whole. A naive approach is to dedicate one issue slot to each thread. Secondly, to avoid structural hazards, we can have four ALUs and allot one ALU to each thread. However, this is a suboptimal utilization of the pipeline because a thread might not have an instruction to issue every cycle. It is best to have a more flexible scheme, where we dynamically partition the issue slots among the threads. This scheme is known as simultaneous multithreading (popularly known as SMT). For example, in a given cycle we might find 2 instructions from thread 2, and 1 instruction each from threads 3, and 4. This situation might reverse in the next cycle. Let us graphically illustrate this concept in Figure 12.18, and simultaneously also compare the SMT approach with fine and coarse-grained multithreading. ",
        "page_idx": 614
    },
    {
        "type": "text",
        "text": "The columns in Figure 12.18 represent the issue slots for a multiple issue machine, and the rows represent the cycles. Instructions belonging to different threads have different colors. Figure 12.18(a) shows the execution of instructions in a coarse-grained machine, where each thread executes for two consecutive cycles. We observe that a lot of issue slots are empty because we do not find a sufficient number of instructions that can execute. Fine-grained multithreading (shown in Figure 12.18(b)) also has the same problem. However, in an SMT processor, we are typically able to keep most of the issue slots busy, because we always find instructions from the set of available threads that are ready to execute. If one thread is stalled for some reason, other threads compensate by executing more instructions. In practice, all the threads do not have low ILP $^ { - 1 }$ phases simultaneously. Hence, the SMT approach has proven to be a very versatile and effective method for leveraging the power of multiple issue processors. Since the Pentium 4 (released in the late nineties), most of the Intel processors support different variants of simultaneous multithreading. In Intel\u2019s terminology SMT is known as hyperthreading . The latest (as of 2012) IBM Power 7 processor has 8 cores, where each core is a 4-way SMT (can run 4 threads per each core). ",
        "page_idx": 614
    },
    {
        "type": "text",
        "text": "Note that the problem of selecting the right set of instructions to issue is very crucial to the performance of an SMT processor. Secondly, the memory bandwidth requirement of an n-way SMT processor is higher than that of an equivalent uniprocessor. The fetch logic is also far more complicated, because now we need to fetch from four separate program counters in the same cycle. Lastly, the issues of maintaining coherence, and consistency further complicate the picture. The reader can refer to the research papers mentioned in the \u201cFurther Reading\u201d section at the end of this chapter. ",
        "page_idx": 614
    }
]