44.9 Mapping Table Size
The second cost of log-structuring is the potential for extremely large mapping tables, with one entry for each 4-KB page of the device. With a large 1-TB SSD, for example, a single 4-byte entry per 4-KB page results in 1 GB of memory needed by the device, just for these mappings! Thus, this page-level FTL scheme is impractical.  
Block-Based Mapping
One approach to reduce the costs of mapping is to only keep a pointer per block of the device, instead of per page, reducing the amount of mapping information by a factor of $\frac { S i z e _ { b l o c k } } { S i z e _ { p a g e } }$ . This block-level FTL is akin to having  
OPERATINGSYSTEMS[VERSION 1.10]  
bigger page sizes in a virtual memory system; in that case, you use fewer bits for the VPN and have a larger offset in each virtual address.  
Unfortunately, using a block-based mapping inside a log-based FTL does not work very well for performance reasons. The biggest problem arises when a “small write” occurs (i.e., one that is less than the size of a physical block). In this case, the FTL must read a large amount of live data from the old block and copy it into a new one (along with the data from the small write). This data copying increases write amplification greatly and thus decreases performance.  
To make this issue more clear, let’s look at an example. Assume the client previously wrote out logical blocks 2000, 2001, 2002, and 2003 (with contents, ${ \sf a } , { \sf b } , { \sf c } , { \sf d } )$ , and that they are located within physical block 1 at physical pages 4, 5, 6, and 7. With per-page mappings, the translation table would have to record four mappings for these logical blocks: $2 0 0 0 {  } 4 ,$ , $2 0 0 1 {  } 5 .$ , $2 0 0 2 {  } 6$ , $2 0 0 3 \substack {  7 }$ .  
If, instead, we use block-level mapping, the FTL only needs to record a single address translation for all of this data. The address mapping, however, is slightly different than our previous examples. Specifically, we think of the logical address space of the device as being chopped into chunks that are the size of the physical blocks within the flash. Thus, the logical block address consists of two portions: a chunk number and an offset. Because we are assuming four logical blocks fit within each physical block, the offset portion of the logical addresses requires 2 bits; the remaining (most significant) bits form the chunk number.  
Logical blocks 2000, 2001, 2002, and 2003 all have the same chunk number (500), and have different offsets (0, 1, 2, and 3, respectively). Thus, with a block-level mapping, the FTL records that chunk 500 maps to block 1 (starting at physical page 4), as shown in this diagram:  
 
In a block-based ${ \mathrm { F T L } } ,$ reading is easy. First, the FTL extracts the chunk number from the logical block address presented by the client, by taking the topmost bits out of the address. Then, the FTL looks up the chunknumber to physical-page mapping in the table. Finally, the FTL computes the address of the desired flash page by adding the offset from the logical address to the physical address of the block.  
For example, if the client issues a read to logical address 2002, the device extracts the logical chunk number (500), looks up the translation in the mapping table (finding 4), and adds the offset from the logical address (2) to the translation (4). The resulting physical-page address (6) is where the data is located; the FTL can then issue the read to that physical address and obtain the desired data (c).  
But what if the client writes to logical block 2002 (with contents $\mathbf { \Lambda } _ { C ^ { \prime } }$ )? In this case, the FTL must read in 2000, 2001, and 2003, and then write out all four logical blocks in a new location, updating the mapping table accordingly. Block 1 (where the data used to reside) can then be erased and reused, as shown here.  
 
As you can see from this example, while block level mappings greatly reduce the amount of memory needed for translations, they cause significant performance problems when writes are smaller than the physical block size of the device; as real physical blocks can be 256KB or larger, such writes are likely to happen quite often. Thus, a better solution is needed. Can you sense that this is the part of the chapter where we tell you what that solution is? Better yet, can you figure it out yourself, before reading on?  
Hybrid Mapping
To enable flexible writing but also reduce mapping costs, many modern FTLs employ a hybrid mapping technique. With this approach, the FTL keeps a few blocks erased and directs all writes to them; these are called log blocks. Because the FTL wants to be able to write any page to any location within the log block without all the copying required by a pure block-based mapping, it keeps per-page mappings for these log blocks.  
The FTL thus logically has two types of mapping table in its memory: a small set of per-page mappings in what we’ll call the log table, and a larger set of per-block mappings in the data table. When looking for a particular logical block, the FTL will first consult the log table; if the logical block’s location is not found there, the FTL will then consult the data table to find its location and then access the requested data.  
The key to the hybrid mapping strategy is keeping the number of log blocks small. To keep the number of log blocks small, the FTL has to periodically examine log blocks (which have a pointer per page) and switch them into blocks that can be pointed to by only a single block pointer. This switch is accomplished by one of three main techniques, based on the contents of the block $[ \mathsf { K K } + \bar { 0 } 2 ]$ .  
For example, let’s say the FTL had previously written out logical pages 1000, 1001, 1002, and 1003, and placed them in physical block 2 (physical  
OPERATINGSYSTEMS[VERSION 1.10]  
pages 8, 9, 10, 11); assume the contents of the writes to 1000, 1001, 1002, and 1003 are a, b, $\scriptstyle \ C ,$ and ${ \mathrm { { d } } } ,$ respectively.  
Log Table: Data Table:250 +8Block:012Page:00 0102 0304 05 06 0708 0910 11Flash dContent:abcChipState:iiiiiiiiVVV
Now assume that the client overwrites each of these blocks (with data $\mathsf { a } ^ { \prime } , \mathsf { b } ^ { \prime } , \mathsf { c } ^ { \prime }$ , and $\mathrm { d } ^ { \prime }$ ), in the exact same order, in one of the currently available log blocks, say physical block 0 (physical pages 0, 1, 2, and 3). In this case, the FTL will have the following state:  
Log Table: Data Table:1000->0 250 +81001-1 1002-21003->3 MemoryBlock: Page:0Flash Chip00 0102 0304 0506 07080910 11Content:a'b'c'd'abcdState:VVVViiiivV
Because these blocks have been written exactly in the same manner as before, the FTL can perform what is known as a switch merge. In this case, the log block (0) now becomes the storage location for blocks 0, 1, 2, and 3, and is pointed to by a single block pointer; the old block (2) is now erased and used as a log block. In this best case, all the per-page pointers required replaced by a single block pointer.  
Log Table: Data Table:250 +0 MemoryBlock:01 04 05 06 072 08 09 10 11Flash ChipPage: Content:00 01 a' b'02 c'03 d'iiiState:vvviiii
This switch merge is the best case for a hybrid FTL. Unfortunately, sometimes the FTL is not so lucky. Imagine the case where we have the same initial conditions (logical blocks 1000 ... 1003 stored in physical block 2) but then the client overwrites logical blocks 1000 and 1001.  
What do you think happens in this case? Why is it more challenging to handle? (think before looking at the result on the next page)  
Log Table: Data Table:1000->0 250 +81001-1 MemoryBlock: Page:01200 0104 05 06 0708 09Flash Chipa'02 03b10 11Content:b'acdState:vv iiiiiivVV
To reunite the other pages of this physical block, and thus be able to refer to them by only a single block pointer, the FTL performs what is called a partial merge. In this operation, logical blocks 1002 and 1003 are read from physical block 2, and then appended to the log. The resulting state of the SSD is the same as the switch merge above; however, in this case, the FTL had to perform extra $\mathrm { I } / \mathrm { O }$ to achieve its goals, thus increasing write amplification.  
The final case encountered by the FTL known as a full merge, and requires even more work. In this case, the FTL must pull together pages from many other blocks to perform cleaning. For example, imagine that logical blocks 0, 4, 8, and 12 are written to log block $A$ . To switch this log block into a block-mapped page, the FTL must first create a data block containing logical blocks 0, 1, 2, and 3, and thus the FTL must read 1, 2, and 3 from elsewhere and then write out 0, 1, 2, and 3 together. Next, the merge must do the same for logical block 4, finding 5, 6, and 7 and reconciling them into a single physical block. The same must be done for logical blocks 8 and 12, and then (finally), the log block $A$ can be freed. Frequent full merges, as is not surprising, can seriously harm performance and thus should be avoided when at all possible $\left[ \mathrm { G } \dot { \mathrm { Y } } { + } 0 9 \right]$ .  
Page Mapping Plus Caching
Given the complexity of the hybrid approach above, others have suggested simpler ways to reduce the memory load of page-mapped FTLs. Probably the simplest is just to cache only the active parts of the FTL in memory, thus reducing the amount of memory needed $\left[ \mathrm { G Y + 0 9 } \right]$ .  
This approach can work well. For example, if a given workload only accesses a small set of pages, the translations of those pages will be stored in the in-memory ${ \mathrm { F T L } } ,$ and performance will be excellent without high memory cost. Of course, the approach can also perform poorly. If memory cannot contain the working set of necessary translations, each access will minimally require an extra flash read to first bring in the missing mapping before being able to access the data itself. Even worse, to make room for the new mapping, the FTL might have to evict an old mapping, and if that mapping is dirty (i.e., not yet written to the flash persistently), an extra write will also be incurred. However, in many cases, the workload will display locality, and this caching approach will both reduce memory overheads and keep performance high.  
OPERATINGSYSTEMS[VERSION 1.10]  