[
    {
        "type": "text",
        "text": "40.7 Caching and Buffering ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "As the examples above show, reading and writing files can be expensive, incurring many I/Os to the (slow) disk. To remedy what would clearly be a huge performance problem, most file systems aggressively use system memory (DRAM) to cache important blocks. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Imagine the open example above: without caching, every file open would require at least two reads for every level in the directory hierarchy (one to read the inode of the directory in question, and at least one to read its data). With a long pathname (e.g., /1/2/3/ ... /100/file.txt), the file system would literally perform hundreds of reads just to open the file! ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Early file systems thus introduced a fixed-size cache to hold popular blocks. As in our discussion of virtual memory, strategies such as LRU and different variants would decide which blocks to keep in cache. This fixed-size cache would usually be allocated at boot time to be roughly $1 0 \\%$ of total memory. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "This static partitioning of memory, however, can be wasteful; what if the file system doesn’t need $1 0 \\%$ of memory at a given point in time? With the fixed-size approach described above, unused pages in the file cache cannot be re-purposed for some other use, and thus go to waste. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Modern systems, in contrast, employ a dynamic partitioning approach. Specifically, many modern operating systems integrate virtual memory pages and file system pages into a unified page cache [S00]. In this way, memory can be allocated more flexibly across virtual memory and file system, depending on which needs more memory at a given time. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Now imagine the file open example with caching. The first open may generate a lot of I/O traffic to read in directory inode and data, but sub",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "OPERATINGSYSTEMS[VERSION 1.10]",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "TIP: UNDERSTAND STATIC VS. DYNAMIC PARTITIONING When dividing a resource among different clients/users, you can use either static partitioning or dynamic partitioning. The static approach simply divides the resource into fixed proportions once; for example, if there are two possible users of memory, you can give some fixed fraction of memory to one user, and the rest to the other. The dynamic approach is more flexible, giving out differing amounts of the resource over time; for example, one user may get a higher percentage of disk bandwidth for a period of time, but then later, the system may switch and decide to give a different user a larger fraction of available disk bandwidth. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Each approach has its advantages. Static partitioning ensures each user receives some share of the resource, usually delivers more predictable performance, and is often easier to implement. Dynamic partitioning can achieve better utilization (by letting resource-hungry users consume otherwise idle resources), but can be more complex to implement, and can lead to worse performance for users whose idle resources get consumed by others and then take a long time to reclaim when needed. As is often the case, there is no best method; rather, you should think about the problem at hand and decide which approach is most suitable. Indeed, shouldn’t you always be doing that? ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "sequent file opens of that same file (or files in the same directory) will mostly hit in the cache and thus no I/O is needed. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Let us also consider the effect of caching on writes. Whereas read I/O can be avoided altogether with a sufficiently large cache, write traffic has to go to disk in order to become persistent. Thus, a cache does not serve as the same kind of filter on write traffic that it does for reads. That said, write buffering (as it is sometimes called) certainly has a number of performance benefits. First, by delaying writes, the file system can batch some updates into a smaller set of $\\mathrm { I / { \\bar { O } s } ; }$ for example, if an inode bitmap is updated when one file is created and then updated moments later as another file is created, the file system saves an I/O by delaying the write after the first update. Second, by buffering a number of writes in memory, the system can then schedule the subsequent I/Os and thus increase performance. Finally, some writes are avoided altogether by delaying them; for example, if an application creates a file and then deletes it, delaying the writes to reflect the file creation to disk avoids them entirely. In this case, laziness (in writing blocks to disk) is a virtue. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "For the reasons above, most modern file systems buffer writes in memory for anywhere between five and thirty seconds, representing yet another trade-off: if the system crashes before the updates have been propagated to disk, the updates are lost; however, by keeping writes in memory longer, performance can be improved by batching, scheduling, and even avoiding writes. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "TIP: UNDERSTAND THE DURABILITY/PERFORMANCE TRADE-OFF Storage systems often present a durability/performance trade-off to users. If the user wishes data that is written to be immediately durable, the system must go through the full effort of committing the newlywritten data to disk, and thus the write is slow (but safe). However, if the user can tolerate the loss of a little data, the system can buffer writes in memory for some time and write them later to the disk (in the background). Doing so makes writes appear to complete quickly, thus improving perceived performance; however, if a crash occurs, writes not yet committed to disk will be lost, and hence the trade-off. To understand how to make this trade-off properly, it is best to understand what the application using the storage system requires; for example, while it may be tolerable to lose the last few images downloaded by your web browser, losing part of a database transaction that is adding money to your bank account may be less tolerable. Unless you’re rich, of course; in that case, why do you care so much about hoarding every last penny? ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Some applications (such as databases) don’t enjoy this trade-off. Thus, to avoid unexpected data loss due to write buffering, they simply force writes to disk, by calling fsync(), by using direct I/O interfaces that work around the cache, or by using the raw disk interface and avoiding the file system altogether2. While most applications live with the tradeoffs made by the file system, there are enough controls in place to get the system to do what you want it to, should the default not be satisfying. ",
        "page_idx": 15
    }
]