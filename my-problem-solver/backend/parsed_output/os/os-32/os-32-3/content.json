[
    {
        "type": "text",
        "text": "32.3 Deadlock Bugs ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Beyond the concurrency bugs mentioned above, a classic problem that arises in many concurrent systems with complex locking protocols is known as deadlock. Deadlock occurs, for example, when a thread (say Thread 1) is holding a lock (L1) and waiting for another one (L2); unfortunately, the thread (Thread 2) that holds lock L2 is waiting for L1 to be released. Here is a code snippet that demonstrates such a potential deadlock: ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Thread 1: pthread_mutex_lock(L1); pthread_mutex_lock(L2); ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Thread 2: pthread_mutex_lock(L2); pthread_mutex_lock(L1); ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Note that if this code runs, deadlock does not necessarily occur; rather, it may occur, if, for example, Thread 1 grabs lock L1 and then a context switch occurs to Thread 2. At that point, Thread 2 grabs L2, and tries to acquire L1. Thus we have a deadlock, as each thread is waiting for the other and neither can run. See Figure 32.7 for a graphical depiction; the presence of a cycle in the graph is indicative of the deadlock. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The figure should make the problem clear. How should programmers write code so as to handle deadlock in some way? ",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/2436911c231548012308130f2975dd3c016c65f0162d83141470189ec3fc27bb.jpg",
        "img_caption": [
            "Figure 32.7: The Deadlock Dependency Graph "
        ],
        "img_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "CRUX: HOW TO DEAL WITH DEADLOCK How should we build systems to prevent, avoid, or at least detect and recover from deadlock? Is this a real problem in systems today? ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Why Do Deadlocks Occur? ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "As you may be thinking, simple deadlocks such as the one above seem readily avoidable. For example, if Thread 1 and 2 both made sure to grab locks in the same order, the deadlock would never arise. So why do deadlocks happen? ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "One reason is that in large code bases, complex dependencies arise between components. Take the operating system, for example. The virtual memory system might need to access the file system in order to page in a block from disk; the file system might subsequently require a page of memory to read the block into and thus contact the virtual memory system. Thus, the design of locking strategies in large systems must be carefully done to avoid deadlock in the case of circular dependencies that may occur naturally in the code. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Another reason is due to the nature of encapsulation. As software developers, we are taught to hide details of implementations and thus make software easier to build in a modular way. Unfortunately, such modularity does not mesh well with locking. As Jula et al. point out $\\left[ \\mathsf { J } { + } 0 8 \\right]$ , some seemingly innocuous interfaces almost invite you to deadlock. For example, take the Java Vector class and the method AddAll(). This routine would be called as follows: ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "OPERATINGSYSTEMS[VERSION 1.10]",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Vector v1, v2;   \nv1.AddAll(v2); ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Internally, because the method needs to be multi-thread safe, locks for both the vector being added to (v1) and the parameter (v2) need to be acquired. The routine acquires said locks in some arbitrary order (say v1 then v2) in order to add the contents of v2 to v1. If some other thread calls v2.AddAll(v1) at nearly the same time, we have the potential for deadlock, all in a way that is quite hidden from the calling application. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Conditions for Deadlock ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Four conditions need to hold for a deadlock to occur $\\scriptstyle { [ C + 7 1 ] }$ : ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "• Mutual exclusion: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).   \n• Hold-and-wait: Threads hold resources allocated to them (e.g., locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).   \n• No preemption: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.   \n• Circular wait: There exists a circular chain of threads such that each thread holds one or more resources (e.g., locks) that are being requested by the next thread in the chain. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "If any of these four conditions are not met, deadlock cannot occur. Thus, we first explore techniques to prevent deadlock; each of these strategies seeks to prevent one of the above conditions from arising and thus is one approach to handling the deadlock problem. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Prevention ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Circular Wait ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Probably the most practical prevention technique (and certainly one that is frequently employed) is to write your locking code such that you never induce a circular wait. The most straightforward way to do that is to provide a total ordering on lock acquisition. For example, if there are only two locks in the system (L1 and L2), you can prevent deadlock by always acquiring L1 before L2. Such strict ordering ensures that no cyclical wait arises; hence, no deadlock. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Of course, in more complex systems, more than two locks will exist, and thus total lock ordering may be difficult to achieve (and perhaps is unnecessary anyhow). Thus, a partial ordering can be a useful way to structure lock acquisition so as to avoid deadlock. An excellent real example of partial lock ordering can be seen in the memory mapping code in Linux $\\left[ \\mathrm { T } { + } 9 4 \\right]$ (v5.2); the comment at the top of the source code reveals ten different groups of lock acquisition orders, including simple ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "TIP: ENFORCE LOCK ORDERING BY LOCK ADDRESS In some cases, a function must grab two (or more) locks; thus, we know we must be careful or deadlock could arise. Imagine a function that is called as follows: do something(mutex t $\\star \\mathrm { m } 1$ , mutex t $\\star \\mathtt { m } 2$ ). If the code always grabs m1 before $\\mathtt { m } 2$ (or always m2 before $\\mathrm { m } 1$ ), it could deadlock, because one thread could call do something(L1, L2) while another thread could call do something(L2, L1). To avoid this particular issue, the clever programmer can use the address of each lock as a way of ordering lock acquisition. By acquiring locks in either high-to-low or low-to-high address order, do something() can guarantee that it always acquires locks in the same order, regardless of which order they are passed in. The code would look something like this: ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "if $( { \\mathrm { m } } 1 { \\mathrm { ~ } } > { \\mathrm { ~ } } { \\mathrm { m } } 2$ ) { // grab in high-to-low address order pthread_mutex_lock(m1); pthread_mutex_lock(m2); else { pthread_mutex_lock(m2); pthread_mutex_lock(m1);   \n// Code assumes that m1 ! $! = { \\textrm { m } } 2 $ (not the same lock) ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "By using this simple technique, a programmer can ensure a simple and efficient deadlock-free implementation of multi-lock acquisition. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "ones such as “i mutex before i mmap rwsem” and more complex orders such as “i mmap rwsem before private lock before swap lock before i pages lock”. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "As you can imagine, both total and partial ordering require careful design of locking strategies and must be constructed with great care. Further, ordering is just a convention, and a sloppy programmer can easily ignore the locking protocol and potentially cause deadlock. Finally, lock ordering requires a deep understanding of the code base, and how various routines are called; just one mistake could result in the $ { ^ { \\prime \\prime } }  { \\mathrm { D } } ^ { \\prime \\prime }$ word1. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Hold-and-wait ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "The hold-and-wait requirement for deadlock can be avoided by acquiring all locks at once, atomically. In practice, this could be achieved as follows: ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "pthread_mutex_lock(prevention); // begin acquisition pthread_mutex_lock(L1);   \npthread_mutex_lock(L2);   \npthread_mutex_unlock(prevention); // end ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "By first grabbing the lock prevention, this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided. Of course, it requires that any time any thread grabs a lock, it first acquires the global prevention lock. For example, if another thread was trying to grab locks L1 and L2 in a different order, it would be OK, because it would be holding the prevention lock while doing so. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Note that the solution is problematic for a number of reasons. As before, encapsulation works against us: when calling a routine, this approach requires us to know exactly which locks must be held and to acquire them ahead of time. This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "No Preemption ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Because we generally view locks as held until unlock is called, multiple lock acquisition often gets us into trouble because when waiting for one lock we are holding another. Many thread libraries provide a more flexible set of interfaces to help avoid this situation. Specifically, the routine pthread mutex trylock() either grabs the lock (if it is available) and returns success or returns an error code indicating the lock is held; in the latter case, you can try again later if you want to grab that lock. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Such an interface could be used as follows to build a deadlock-free, ordering-robust lock acquisition protocol: ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "1 top:   \n2 pthread_mutex_lock(L1);   \n3 if (pthread_mutex_trylock(L2) ! $\\ ! = \\begin{array} { l l } { 0 } \\end{array}$ ) {   \n4 pthread_mutex_unlock(L1);   \n5 goto top;   \n6 } ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Note that another thread could follow the same protocol but grab the locks in the other order (L2 then L1) and the program would still be deadlock free. One new problem does arise, however: livelock. It is possible (though perhaps unlikely) that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks. In this case, both systems are running through this code sequence over and over again (and thus it is not a deadlock), but progress is not being made, hence the name livelock. There are solutions to the livelock problem, too: for example, one could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "One point about this solution: it skirts around the hard parts of using a trylock approach. The first problem that would likely exist again arises due to encapsulation: if one of these locks is buried in some routine that is getting called, the jump back to the beginning becomes more complex to implement. If the code had acquired some resources (other than L1) ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "along the way, it must make sure to carefully release them as well; for example, if after acquiring L1, the code had allocated some memory, it would have to release that memory upon failure to acquire L2, before jumping back to the top to try the entire sequence again. However, in limited circumstances (e.g., the Java vector method mentioned earlier), this type of approach could work well. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "You might also notice that this approach doesn’t really add preemption (the forcible action of taking a lock away from a thread that owns it), but rather uses the trylock approach to allow a developer to back out of lock ownership (i.e., preempt their own ownership) in a graceful way. However, it is a practical approach, and thus we include it here, despite its imperfection in this regard. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Mutual Exclusion ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "The final prevention technique would be to avoid the need for mutual exclusion at all. In general, we know this is difficult, because the code we wish to run does indeed have critical sections. So what can we do? ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Herlihy had the idea that one could design various data structures without locks at all [H91, H93]. The idea behind these lock-free (and related wait-free) approaches here is simple: using powerful hardware instructions, you can build data structures in a manner that does not require explicit locking. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "As a simple example, let us assume we have a compare-and-swap instruction, which as you may recall is an atomic instruction provided by the hardware that does the following: ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "1 int CompareAndSwap(int $\\star$ address, int expected, int new) {   \n2 if (\\*address $\\scriptstyle = =$ expected) {   \n3 \\*address $\\mathbf { \\tau } =$ new;   \n4 return 1; // success   \n5 }   \n6 return 0; // failure   \n7 } ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Imagine we now wanted to atomically increment a value by a certain amount, using compare-and-swap. We could do so with the following simple function: ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "1 void AtomicIncrement(int \\*value, int amount) {   \n2 do {   \n3 int old $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ \\*value;   \n4 } while (CompareAndSwap(value, old, old $^ +$ amount) $\\scriptstyle \\mathbf { \\mu } = \\mathbf { \\mu } 0$ );   \n5 } ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Instead of acquiring a lock, doing the update, and then releasing it, we have instead built an approach that repeatedly tries to update the value to the new amount and uses the compare-and-swap to do so. In this manner, ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "OPERATINGSYSTEMS[VERSION 1.10]",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "no lock is acquired, and no deadlock can arise (though livelock is still a possibility, and thus a robust solution will be more complex than the simple code snippet above). ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Let us consider a slightly more complex example: list insertion. Here is code that inserts at the head of a list: ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "1 void insert(int value) {   \n2 node_t $\\begin{array} { l l } { \\star \\boldsymbol { \\mathrm { n } } } & { = } \\end{array}$ malloc(sizeof(node_t));   \n3 assert(n ! $! =$ NULL);   \n4 $\\mathrm { n - } >$ value $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ value;   \n5 n->next $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ head;   \n6 head $\\mathbf { \\Omega } = \\mathbf { \\Omega } _ { \\mathrm { ~ n ~ } }$ ;   \n7 } ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "This code performs a simple insertion, but if called by multiple threads at the “same time”, has a race condition. Can you figure out why? (draw a picture of what could happen to a list if two concurrent insertions take place, assuming, as always, a malicious scheduling interleaving). Of course, we could solve this by surrounding this code with a lock acquire and release: ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "1 void insert(int value) {   \n2 node_t $\\begin{array} { l l } { \\star \\boldsymbol { \\mathrm { n } } } & { = } \\end{array}$ malloc(sizeof(node_t));   \n3 assert(n ! $\\ O =$ NULL);   \n4 $\\mathrm { n - } >$ value $\\mathbf { \\tau } = \\mathbf { \\tau }$ value;   \n5 pthread_mutex_lock(listlock); // begin critical section   \n6 n->next $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ head;   \n7 head $\\mathbf { \\Sigma } = \\mathbf { \\Sigma } _ { \\mathrm { n } }$ ;   \n8 pthread_mutex_unlock(listlock); // end critical section   \n9 } ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "In this solution, we are using locks in the traditional manner2. Instead, let us try to perform this insertion in a lock-free manner simply using the compare-and-swap instruction. Here is one possible approach: ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "1 void insert(int value) {   \n2 node_t $\\begin{array} { l l } { \\star \\mathsf { n } } & { = } \\end{array}$ malloc(sizeof(node_t));   \n3 assert(n ! $\\ O =$ NULL);   \n4 $\\mathrm { n - } >$ value $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ value;   \n5 do {   \n6 n->next $\\mathbf { \\Sigma } =$ head;   \n7 } while (CompareAndSwap(&head, n->next, n) == 0);   \n8 } ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "The code here updates the next pointer to point to the current head, and then tries to swap the newly-created node into position as the new head of the list. However, this will fail if some other thread successfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Of course, building a useful list requires more than just a list insert, and not surprisingly building a list that you can insert into, delete from, and perform lookups on in a lock-free manner is non-trivial. Read the rich literature on lock-free and wait-free synchronization to learn more [H01, H91, H93]. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Deadlock Avoidance via Scheduling ",
        "text_level": 1,
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Instead of deadlock prevention, in some scenarios deadlock avoidance is preferable. Avoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently schedules said threads in a way as to guarantee no deadlock can occur. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "For example, assume we have two processors and four threads which must be scheduled upon them. Assume further we know that Thread 1 (T1) grabs locks L1 and L2 (in some order, at some point during its execution), T2 grabs L1 and L2 as well, T3 grabs just L2, and T4 grabs no locks at all. We can show these lock acquisition demands of the threads in tabular form: ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "A smart scheduler could thus compute that as long as T1 and T2 are not run at the same time, no deadlock could ever arise. Here is one such schedule: ",
        "page_idx": 11
    },
    {
        "type": "image",
        "img_path": "images/ac0bc7b3cc438bfcbc9e8ffe22497fa60e70bcb510326b58c7c8deb69fc1e6c9.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Note that it is OK for (T3 and T1) or (T3 and T2) to overlap. Even though T3 grabs lock L2, it can never cause a deadlock by running concurrently with other threads because it only grabs one lock. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Let’s look at one more example. In this one, there is more contention for the same resources (again, locks L1 and L2), as indicated by the following contention table: ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "OPERATINGSYSTEMS[VERSION 1.10]",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "TIP: DON’T ALWAYS DO IT PERFECTLY (TOM WEST’S LAW) Tom West, famous as the subject of the classic computer-industry book Soul of a New Machine [K81], says famously: “Not everything worth doing is worth doing well”, which is a terrific engineering maxim. If a bad thing happens rarely, certainly one should not spend a great deal of effort to prevent it, particularly if the cost of the bad thing occurring is small. If, on the other hand, you are building a space shuttle, and the cost of something going wrong is the space shuttle blowing up, well, perhaps you should ignore this piece of advice. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Some readers object: “This sounds like you are suggesting mediocrity as a solution!” Perhaps they are right, that we should be careful with advice such as this. However, our experience tells us that in the world of engineering, with pressing deadlines and other real-world concerns, one will always have to decide which aspects of a system to build well and which to put aside for another day. The hard part is knowing which to do when, a bit of insight only gained through experience and dedication to the task at hand. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "In particular, threads T1, T2, and T3 all need to grab both locks L1 and L2 at some point during their execution. Here is a possible schedule that guarantees that no deadlock could ever occur: ",
        "page_idx": 12
    },
    {
        "type": "image",
        "img_path": "images/6301ebb8ac143c6f4be3b089325545f0ea167b8d29920d18411d1fc1622ea1c9.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "As you can see, static scheduling leads to a conservative approach where T1, T2, and T3 are all run on the same processor, and thus the total time to complete the jobs is lengthened considerably. Though it may have been possible to run these tasks concurrently, the fear of deadlock prevents us from doing so, and the cost is performance. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "One famous example of an approach like this is Dijkstra’s Banker’s Algorithm [D64], and many similar approaches have been described in the literature. Unfortunately, they are only useful in very limited environments, for example, in an embedded system where one has full knowledge of the entire set of tasks that must be run and the locks that they need. Further, such approaches can limit concurrency, as we saw in the second example above. Thus, avoidance of deadlock via scheduling is not a widely-used general-purpose solution. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Detect and Recover ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "One final general strategy is to allow deadlocks to occasionally occur, and then take some action once such a deadlock has been detected. For example, if an OS froze once a year, you would just reboot it and get happily (or grumpily) on with your work. If deadlocks are rare, such a non-solution is indeed quite pragmatic. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Many database systems employ deadlock detection and recovery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted. If more intricate repair of data structures is first required, a human being may be involved to ease the process. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "More detail on database concurrency, deadlock, and related issues can be found elsewhere $[ \\mathrm { B } \\substack { + 8 7 }$ , K87]. Read these works, or better yet, take a course on databases to learn more about this rich and interesting topic. ",
        "page_idx": 13
    }
]