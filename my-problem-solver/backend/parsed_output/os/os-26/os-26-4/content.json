[
    {
        "type": "text",
        "text": "26.4 The Heart Of The Problem: Uncontrolled Scheduling ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "To understand why this happens, we must understand the code sequence that the compiler generates for the update to counter. In this case, we wish to simply add a number (1) to counter. Thus, the code sequence for doing so might look something like this (in x86); ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "mov 0x8049a1c, %eax add $\\$ 0 x1$ , %eax mov %eax, 0x8049a1c ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "This example assumes that the variable counter is located at address $0 \\times 8 0 4 9 \\mathrm { a } 1 \\mathrm { c }$ . In this three-instruction sequence, the $\\boldsymbol { \\times } 8 6 \\mathrm { ~ m o v ~ }$ instruction is used first to get the memory value at the address and put it into register eax. Then, the add is performed, adding 1 (0x1) to the contents of the eax register, and finally, the contents of eax are stored back into memory at the same address. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Let us imagine one of our two threads (Thread 1) enters this region of code, and is thus about to increment counter by one. It loads the value of counter (let’s say it’s 50 to begin with) into its register eax. Thus, $\\mathtt { e a x } = 5 0$ for Thread 1. Then it adds one to the register; thus $\\mathtt { e a x } = 5 1$ . Now, something unfortunate happens: a timer interrupt goes off; thus, the OS saves the state of the currently running thread (its ${ \\bar { \\mathrm { P C } } } ,$ its registers including eax, etc.) to the thread’s TCB. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Now something worse happens: Thread 2 is chosen to run, and it enters this same piece of code. It also executes the first instruction, getting the value of counter and putting it into its eax (remember: each thread when running has its own private registers; the registers are virtualized by the context-switch code that saves and restores them). The value of counter is still 50 at this point, and thus Thread 2 has $\\mathtt { e a x } = 5 0$ . Let’s then assume that Thread 2 executes the next two instructions, incrementing eax by 1 (thus $\\mathtt { e a x } = 5 1$ ), and then saving the contents of eax into counter (address $0 { \\times } 8 0 4 9 { \\mathrm { a } } 1 { \\mathrm { c } }$ ). Thus, the global variable counter now has the value 51. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Finally, another context switch occurs, and Thread 1 resumes running. Recall that it had just executed the mov and add, and is now about to perform the final mov instruction. Recall also that $\\mathtt { e a x } = 5 1$ . Thus, the final mov instruction executes, and saves the value to memory; the counter is set to 51 again. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Put simply, what has happened is this: the code to increment counter has been run twice, but counter, which started at 50, is now only equal to 51. A “correct” version of this program should have resulted in the variable counter equal to 52. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Let’s look at a detailed execution trace to understand the problem better. Assume, for this example, that the above code is loaded at address 100 in memory, like the following sequence (note for those of you used to nice, RISC-like instruction sets: $\\bar { \\bf x } 8 6$ has variable-length instructions; this mov instruction takes up 5 bytes of memory, and the add only 3): ",
        "page_idx": 8
    },
    {
        "type": "table",
        "img_path": "images/e3359ad3feb6307880c4844fd12c77a26b8823b53b9fbbd315ef72e7406defbf.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>os</td><td>Thread 1</td><td>Thread 2.</td><td>PC</td><td>(after instruction)</td><td>eax counter</td></tr><tr><td></td><td>before critical section</td><td></td><td>100</td><td>0</td><td>50</td></tr><tr><td></td><td>mov 8049a1c,%eax</td><td></td><td>105</td><td> 50</td><td>50</td></tr><tr><td></td><td>add $0xl,%eax</td><td></td><td>108</td><td>51</td><td>50</td></tr><tr><td>interrupt save T1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>restore T2</td><td></td><td></td><td>100</td><td>0</td><td>50</td></tr><tr><td></td><td></td><td>mov 8049a1c,%eax</td><td>105</td><td> 50</td><td>50</td></tr><tr><td></td><td></td><td>add $0x1,%eax</td><td>108</td><td>51</td><td>50</td></tr><tr><td></td><td></td><td>mov %eax,8049a1c</td><td>113</td><td>51</td><td>51</td></tr><tr><td>interrupt</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>save T2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>restore T1</td><td></td><td></td><td>108</td><td>51</td><td>51</td></tr><tr><td></td><td>mov %eax,8049a1c</td><td></td><td>113</td><td>51</td><td>51</td></tr></table></body></html>\n\n",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "100 mov 0x8049a1c, %eax   \n105 add $\\$ 0 x1$ , %eax   \n108 mov %eax, 0x8049a1c ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "With these assumptions, what happens is shown in Figure 26.7 (page 10). Assume the counter starts at value 50, and trace through this example to make sure you understand what is going on. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "What we have demonstrated here is called a race condition (or, more specifically, a data race): the results depend on the timing of the code’s execution. With some bad luck (i.e., context switches that occur at untimely points in the execution), we get the wrong result. In fact, we may get a different result each time; thus, instead of a nice deterministic computation (which we are used to from computers), we call this result indeterminate, where it is not known what the output will be and it is indeed likely to be different across runs. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Because multiple threads executing this code can result in a race condition, we call this code a critical section. A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Virtually all of these terms, by the way, were coined by Edsger Dijkstra, who was a pioneer in the field and indeed won the Turing Award because of this and other work; see his 1968 paper on “Cooperating Sequential Processes” [D68] for an amazingly clear description of the problem. We’ll be hearing more about Dijkstra in this section of the book. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "OPERATINGSYSTEMS[VERSION 1.10]",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "TIP: USE ATOMIC OPERATIONS",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Atomic operations are one of the most powerful underlying techniques in building computer systems, from the computer architecture, to concurrent code (what we are studying here), to file systems (which we’ll study soon enough), database management systems, and even distributed systems $[ { \\mathrm { L } } + 9 3 ]$ . ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "The idea behind making a series of actions atomic is simply expressed with the phrase “all or nothing”; it should either appear as if all of the actions you wish to group together occurred, or that none of them occurred, with no in-between state visible. Sometimes, the grouping of many actions into a single atomic action is called a transaction, an idea developed in great detail in the world of databases and transaction processing [GR92]. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "In our theme of exploring concurrency, we’ll be using synchronization primitives to turn short sequences of instructions into atomic blocks of execution, but the idea of atomicity is much bigger than that, as we will see. For example, file systems use techniques such as journaling or copyon-write in order to atomically transition their on-disk state, critical for operating correctly in the face of system failures. If that doesn’t make sense, don’t worry — it will, in some future chapter. ",
        "page_idx": 10
    }
]