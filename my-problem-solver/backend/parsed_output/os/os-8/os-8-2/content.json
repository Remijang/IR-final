[
    {
        "type": "text",
        "text": "8.2 Attempt #1: How To Change Priority ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We now must decide how MLFQ is going to change the priority level of a job (and thus which queue it is on) over the lifetime of a job. To do this, we must keep in mind our workload: a mix of interactive jobs that are short-running (and may frequently relinquish the CPU), and some longer-running “CPU-bound” jobs that need a lot of CPU time but where response time isn’t important. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "For this, we need a new concept, which we will call the job’s allotment. The allotment is the amount of time a job can spend at a given priority level before the scheduler reduces its priority. For simplicity, at first, we will assume the allotment is equal to a single time slice. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Here is our first attempt at a priority-adjustment algorithm: ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "• Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue).   \n• Rule 4a: If a job uses up its allotment while running, its priority is reduced (i.e., it moves down one queue).   \n• Rule 4b: If a job gives up the CPU (for example, by performing an I/O operation) before the allotment is up, it stays at the same priority level (i.e., its allotment is reset). ",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/ee422d4c8a2c60c4a98958f28aa81c564525ca23ef7b4f79be83ebe0360b240f.jpg",
        "img_caption": [
            "Figure 8.2: Long-running Job Over Time "
        ],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Example 1: A Single Long-Running Job ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Let’s look at some examples. First, we’ll look at what happens when there has been a long running job in the system, with a time slice of $1 0 \\mathrm { m s }$ (and with the allotment set equal to the time slice). Figure 8.2 shows what happens to this job over time in a three-queue scheduler. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "As you can see in the example, the job enters at the highest priority (Q2). After a single time slice of $1 0 ~ \\mathrm { m s } ,$ , the scheduler reduces the job’s priority by one, and thus the job is on Q1. After running at Q1 for a time slice, the job is finally lowered to the lowest priority in the system (Q0), where it remains. Pretty simple, no? ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Example 2: Along Came A Short Job ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Now let’s look at a more complicated example, and hopefully see how MLFQ tries to approximate SJF. In this example, there are two jobs: A, which is a long-running CPU-intensive job, and B, which is a short-running interactive job. Assume A has been running for some time, and then B arrives. What will happen? Will MLFQ approximate SJF for B? ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Figure 8.3 on page 5 (left) plots the results of this scenario. Job A (shown in black) is running along in the lowest-priority queue (as would any long-running CPU-intensive jobs); B (shown in gray) arrives at time $T \\stackrel { \\cdot } { = } 1 0 \\bar { 0 } .$ , and thus is inserted into the highest queue; as its run-time is short (only $2 0 \\mathrm { m s }$ ), B completes before reaching the bottom queue, in two time slices; then A resumes running (at low priority). ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "From this example, you can hopefully understand one of the major goals of the algorithm: because it doesn’t know whether a job will be a short job or a long-running job, it first assumes it might be a short job, thus giving the job high priority. If it actually is a short job, it will run quickly and complete; if it is not a short job, it will slowly move down the queues, and thus soon prove itself to be a long-running more batch-like process. In this manner, MLFQ approximates SJF. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "OPERATINGSYSTEMS[VERSION 1.10]",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/8206527d058e1dd007afab3d328cb0eab03659cb6e85cfb3f59cd8f379981c97.jpg",
        "img_caption": [
            "Figure 8.3: Along Came An Interactive Job: Two Examples "
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Example 3: What About I/O? ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Let’s now look at an example with some I/O. As Rule 4b states above, if a process gives up the processor before using up its allotment, we keep it at the same priority level. The intent of this rule is simple: if an interactive job, for example, is doing a lot of I/O (say by waiting for user input from the keyboard or mouse), it will relinquish the CPU before its allotment is complete; in such case, we don’t wish to penalize the job and thus simply keep it at the same level. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Figure 8.3 (right) shows an example of how this works, with an interactive job B (shown in gray) that needs the CPU only for $1 \\mathrm { m s }$ before performing an I/O competing for the CPU with a long-running batch job A (shown in black). The MLFQ approach keeps B at the highest priority because B keeps releasing the CPU; if B is an interactive job, MLFQ further achieves its goal of running interactive jobs quickly. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Problems With Our Current MLFQ ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "We thus have a basic MLFQ. It seems to do a fairly good job, sharing the CPU fairly between long-running jobs, and letting short or I/O-intensive interactive jobs run quickly. Unfortunately, the approach we have developed thus far contains serious flaws. Can you think of any? ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "(This is where you pause and think as deviously as you can) ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "First, there is the problem of starvation: if there are “too many” interactive jobs in the system, they will combine to consume all CPU time, and thus long-running jobs will never receive any CPU time (they starve). We’d like to make some progress on these jobs even in this scenario. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Second, a smart user could rewrite their program to game the scheduler. Gaming the scheduler generally refers to the idea of doing something sneaky to trick the scheduler into giving you more than your fair share of the resource. The algorithm we have described is susceptible to ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "TIP: SCHEDULING MUST BE SECURE FROM ATTACK You might think that a scheduling policy, whether inside the OS itself (as discussed herein), or in a broader context (e.g., in a distributed storage system’s I/O request handling $[ \\Upsilon + 1 8 ] _ { . } ^ { \\cdot }$ ), is not a security concern, but in increasingly many cases, it is exactly that. Consider the modern datacenter, in which users from around the world share CPUs, memories, networks, and storage systems; without care in policy design and enforcement, a single user may be able to adversely harm others and gain advantage for itself. Thus, scheduling policy forms an important part of the security of a system, and should be carefully constructed. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "the following attack: before the allotment is used, issue an I/O operation (e.g., to a file) and thus relinquish the CPU; doing so allows you to remain in the same queue, and thus gain a higher percentage of CPU time. When done right (e.g., by running for $9 9 \\%$ of the allotment before relinquishing the CPU), a job could nearly monopolize the CPU. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Finally, a program may change its behavior over time; what was CPUbound may transition to a phase of interactivity. With our current approach, such a job would be out of luck and not be treated like the other interactive jobs in the system. ",
        "page_idx": 5
    }
]