B.4 Computation on a GPU

The graphics processing model is actually a combination of multi-threading, multi-programming, and SIMD execution. NVIDIA calls its model SIMT (Single Instruction, Multi-threaded). Let us look at NVIDIA’s SIMT execution model.

The programmer starts out by writing code in the CUDA programming language. CUDA stands for Compute Unified Device Architecture. It is a custom extension to C/C++ that is compiled by NVIDIA’s nvcc compiler to generate code in both the CPU’s ISA (for the CPU), and in the PTX instruction set (for the GPU). A CUDA program contains a set of kernels that run on the GPU and a set of functions that run on the host CPU. The functions on the host CPU transfer data to and from the GPU, initialize the variables, and co-ordinate the execution of kernels on the GPU. A kernel is defined as a function that executes in parallel on the GPU. The graphics hardware creates multiple copies of each CUDA kernel, and each copy executes on a separate thread.

The GPU maps each such thread to an SP core. It is possible to seamlessly create and execute hundreds of threads for a single CUDA kernel. An astute reader might argue that if the code is the same for multiple copies then what is the point of running multiple copies. Well, the answer is that the code is not exactly the same. The code implicitly takes the id of the thread as an input. For example, if we generate 100 threads for each CUDA kernel, then each thread has a unique id in the set  . Based on the id of the thread, the code in the CUDA kernel performs appropriate processing. Recall that we had seen a very similar example, when we had written OpenMP programs (see Example 149). Now, it is possible that the threads of many applications might be running at the same time. The MT issue logic of each SM schedules the threads and co-ordinates their execution. An SM in this architecture can handle up to 768 threads.

If we are running multiple applications in parallel then the GPU as a whole will need to schedule thousands of threads. The scheduling overhead is prohibitive. Hence, to make the task of scheduling simpler, the GeForce 8800 GPU groups a set of 32 threads into a warp. Each SM can manage 24 warps. A warp is an atomic unit of threads, and either all the threads of a warp are scheduled, or no thread in the warp is scheduled. Moreover, all the threads in a warp belong to the same kernel, and start at exactly the same address. However, after they have started they can have different program counters.

Each SM maps the threads of a warp to SP cores. It executes the warp instruction by instruction. This is similar to classic SIMD execution, where we execute one instruction on multiple data streams, and then move to the next instruction. The SM executes an instruction for each thread in the warp, and after all the threads have completed the instruction, it executes the next instruction. If the kernel has a branch that is data or thread dependent, then the SM executes instructions for only those threads that have instructions in the correct branch path. The GeForce GPU uses predicated instructions. For the instructions on the wrong path, the predicated condition is false. Hence, these instructions are dynamically replaced with nop instructions. Once the branch paths (taken, and not taken) reconverge, all the threads in a warp become active again. The main difference from the SIMD model is that in a SIMD processor, the same thread handles multiple data streams in the same instruction. Whereas, in this case, the same instruction is executed in multiple threads, and each instruction operates on different data streams. After executing an instruction in a warp the MT issue unit might schedule the same warp, another warp from the same application, or a warp from another application. The GPU essentially implements fine grained multithreading at the level of warps. Figure B.4 shows an example.

For executing, a 32 thread warp, an SM typically uses 4 cycles. In the first cycle, it issues 8 threads to each of the 8 SP cores. In the second cycle, it issues 8 more threads to the SFUs. Since the two SFUs have 4 functional units each, they can process 8 instructions in parallel without any structural hazards. In the third cycle, 8 more threads, are sent to the SP cores, and finally in the fourth cycle, 8 threads are sent to the two SFU cores. This strategy of switching between using SFUs, and SP cores, ensures that both the units are kept busy. Since a warp is an atomic unit, it cannot be split across SMs, and each instruction of the warp must finish executing for all the active threads, before we can execute the next instruction in the warp. We can conceptually equate the concept of warps to a 32 lane wide SIMD machine. Multiple warps in the same application can execute independently. To synchronize between warps we need to use global memory, or sophisticated synchronization primitives available in modern GPUs.