[
    {
        "type": "text",
        "text": "12.4.3 Memory Consistency ",
        "text_level": 1,
        "page_idx": 585
    },
    {
        "type": "text",
        "text": "Overview ",
        "text_level": 1,
        "page_idx": 585
    },
    {
        "type": "text",
        "text": "Coherence was all about accesses to the same memory location. What about access to different memory locations? Let us explain with a series of examples. ",
        "page_idx": 585
    },
    {
        "type": "table",
        "img_path": "images/e44b900849e0c312b39ce22f19a56b5a15984ac0a119c018ab725de34f3fc5de.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td colspan=\"2\">Example 152</td></tr><tr><td>Thread 1:</td><td>Thread 2:</td></tr><tr><td>x=1</td><td>t1= y</td></tr><tr><td>y=1</td><td>t2 = x</td></tr><tr><td></td><td></td></tr></table></body></html>\n\n",
        "page_idx": 585
    },
    {
        "type": "text",
        "text": "Let us look at the permissible values of $t 1$ , and $t 2$ from an intuitive standpoint. We can always read (t1,t2)=(0,0). This can happen when thread 2 is scheduled before thread 1. We can also read $( t 1 , t 2 )$ =(1,1). This will happen when thread 2 is scheduled after thread 1 finishes. Likewise, it is possible to read $( t 1 , t 2 )$ =(0,1). Figure 12.6 shows how we can get all the three outcomes. ",
        "page_idx": 585
    },
    {
        "type": "image",
        "img_path": "images/649b197549055bb481290d41708d7a6990a18e872d2946d9025f61a52b6dfe70.jpg",
        "img_caption": [
            "Figure 12.6: Graphical representation of all the possible outcomes "
        ],
        "img_footnote": [],
        "page_idx": 585
    },
    {
        "type": "text",
        "text": "The interesting question is whether $( t 1 , t 2 )$ =(1,0) is allowed? This will happen when the write to $x$ is somehow delayed by the memory system, whereas the write to $y$ completes quickly. In this case $t 1$ will get the updated value of $y$ , and $t 2$ will get the old value of $x$ . The question is whether such behavior should be allowed. Clearly if this is allowed, it will become hard to reason about software and the correctness of parallel algorithms. It will also become hard to program because the behavior appears non-intuitive. However, if we allow such behavior then our hardware design becomes simpler because we do not have to provide strong guarantees to software. ",
        "page_idx": 585
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "There is clearly no right or wrong answer? It all depends on how we want to program software, and what hardware designers want to build for software writers. But, still there is something very profound about this example, and the special case of $( t 1 , t 2 )$ equal to (1,0). To find out why, let us take a look again at Figure 12.6. In this figure, we have been able to reason about three outcomes by creating an interleaving between the instructions of the two threads. In each of these interleavings, the order of instructions in the same thread is the same as the way it is specified in the program. This is known as program order. ",
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "Definition 123 ",
        "text_level": 1,
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "An order of instructions (possibly belonging to multiple threads) that is consistent with the control-flow semantics of each constituent thread is said to be in program order. The controlflow semantics of a thread is defined as the set of rules that determine which instructions can execute after a given instruction. For example, the set of instructions executed by a single cycle processor is always in program order. ",
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "Observation: It is clear that we cannot generate the outcome $( t 1 , t 2 )$ =(1,0) by interleaving threads in program order. ",
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "It would be nice if we can somehow exclude the output (1,0) from the set of possible (allowed) outputs. It will allow us to write parallel software, where we can predict the possible outcomes very easily. A model of the memory system that determines the set of possible outcomes for parallel programs is known as a memory model. ",
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "Definition 124 ",
        "text_level": 1,
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "The model of a memory system that determines the set of likely outcomes for parallel programs is known as a memory model. ",
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "Sequential Consistency ",
        "text_level": 1,
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "We can have different kinds of memory models corresponding to different kinds of processors. One of the most important memory models is known as sequential consistency(SC). Sequential consistency states that only those outcomes are allowed that can be generated by an interleaving of threads in program order. This means that all the outcomes shown in Figure 12.6 are allowed because they are generated by interleaving thread 1 and 2 in all possible ways, without violating their program order. However, the outcome $( t 1 , t 2 )$ =(1,0) is not allowed because it violates program order. Hence, it is not allowed in a sequentially consistent memory model. Note that once we interleave multiple threads in program order, it is the same as saying that we have one processor that executes an instruction from one thread in one cycle and possibly another instruction from some other thread in the next cycle. However, the program order (control flow semantics) of each thread is preserved. Hence, a uniprocessor processing multiple threads produces an SC execution. In fact, if we think about the name of the model, the word \u201csequential\u201d comes from the notion that the execution is equivalent to a uniprocessor sequentially executing the instructions of all the threads in some order that is consistent with each thread\u2019s program order. ",
        "page_idx": 586
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 587
    },
    {
        "type": "text",
        "text": "Definition 125 ",
        "text_level": 1,
        "page_idx": 587
    },
    {
        "type": "text",
        "text": "A memory model is sequentially consistent if the outcome of the execution of a set of parallel threads is equivalent to that of a single processor executing instructions from all the threads in some order. Alternatively, we can define sequential consistency as a memory model whose set of possible outcomes are those that can be generated by interleaving a set of threads in program order. ",
        "page_idx": 587
    },
    {
        "type": "text",
        "text": "Sequential consistency is a very important concept and is widely studied in the fields of computer architecture and distributed systems. It reduces a parallel system to an equivalent uniprocessor system with one processor by equating the execution on a parallel system with the execution on a sequential system. An important point to note is that SC does not mean that the outcome of the execution of a set of parallel programs is the same all the time. This depends on the way that the threads are interleaved and the relative delays f the threads. All that it says is that certain outcomes are not allowed. ",
        "page_idx": 587
    },
    {
        "type": "text",
        "text": "Weak Consistency (WC)\\* ",
        "text_level": 1,
        "page_idx": 587
    },
    {
        "type": "text",
        "text": "The implementation of SC comes at a cost. It makes software simple, but it makes hardware very slow. To support SC, it is often necessary to wait for a read or write to complete, before the next read or write can be sent to the memory system. A write request $W$ completes when all subsequent reads by any processor are guaranteed to get the value written by $W$ , or the value written by a later write to the same location. A read request completes, after it reads the data, and the write request that originally wrote the data completes. ",
        "page_idx": 587
    },
    {
        "type": "text",
        "text": "These requirements/restrictions become a bottleneck in high-performance systems. Hence, the computer architecture community has moved to weak memory models that violate SC. A weak memory model will allow the outcome $( t 1 , t 2 )$ =(1,0) in the following multithreaded code snippet. ",
        "page_idx": 587
    },
    {
        "type": "text",
        "text": "Thread 1: Thread 2: $\\begin{array} { r } { \\mathrm { ~  ~ x ~ } = \\mathrm { ~  ~ 1 ~ } } \\\\ { \\mathrm { ~  ~ y ~ } = \\mathrm { ~  ~ 1 ~ } } \\end{array}$ $\\begin{array} { r } { \\mathrm { t } 1 \\ = \\ \\mathrm { y } } \\\\ { \\mathrm { t } 2 \\ = \\ \\mathrm { x } } \\end{array}$ ",
        "page_idx": 587
    },
    {
        "type": "text",
        "text": "Definition 126 ",
        "text_level": 1,
        "page_idx": 588
    },
    {
        "type": "text",
        "text": "A weakly consistent (WC) memory model does not obey SC. It typically allows arbitrary memory orderings. ",
        "page_idx": 588
    },
    {
        "type": "text",
        "text": "There are different kinds of weak memory models. Let us look at a generic variant, and call it weak consistency (WC). Let us now try to find out why WC allows the (1,0) outcome. Assume that thread 1 is running on core 1, and thread 2 is running on core 2. Moreover, assume that the memory location corresponding to $x$ is near core 2, and the memory location corresponding to $y$ is near core 1. Also assume that it takes tens of cycles to send a request from the vicinity of core 1 to core 2, and the delay is variable. Let us first investigate the behavior of the pipeline of core 1. From the point of view of the pipeline of core 1, once a memory write request is handed over to the memory system, the memory write instruction is deemed to have finished. The instruction moves on to the RW stage. Hence, in this case, the processor will hand over the write to $x$ to the memory system in the $n ^ { t h }$ cycle, and subsequently pass on the write to $y$ in the $( n + 1 ) ^ { t h }$ cycle. The write to $y$ will reach the memory location of $y$ quickly, while the write to $x$ will take a long time. ",
        "page_idx": 588
    },
    {
        "type": "text",
        "text": "In the meanwhile, core 2 (running thread 2) will try to read the value of $y$ . Assume that the read request arrives at the memory location of $y$ just after the write request (to $y$ ) reaches it. Thus, we will get the new value of $y$ , which is equal to 1. Subsequently, core 2 will issue a read to $x$ . It is possible that the read to $x$ reaches the memory location of $x$ just before the write to $x$ reaches it. In this case, it will fetch the old value of $x$ , which is $0$ . Thus, the outcome (1,0) is possible in a weak memory model. ",
        "page_idx": 588
    },
    {
        "type": "text",
        "text": "Now, to avoid this situation, we could have waited for the write to $x$ to complete fully, before issuing the write request to $y$ . It is true that in this case, this would have been the right thing to do. However, in general, when we are writing to shared memory locations, other threads are not reading them at exactly the same point of time. We have no way of distinguishing both the situations at runtime since processors do not share their memory access patterns between each other. Hence, in the interest of performance, it is not worthwhile to delay every memory request till the previous memory requests complete. High-performance implementations thus prefer memory models that allow memory accesses from the same thread to be reordered by the memory system. Note that we shall investigate ways of avoiding the (1,0) outcome in the next subsection. ",
        "page_idx": 588
    },
    {
        "type": "text",
        "text": "Let us summarize our discussion that we have had on weak memory models by defining the assumptions that most processors make. Most processors assume that a memory request completes instantaneously at some point of time after it leaves the pipeline. Furthermore, all the threads assume that a memory request completes instantaneously at exactly the same point of time. This property of a memory request is known as atomicity. Second, we need to note that the order of completion of memory requests might differ from their program order. When the order of completion is the same as the program order of each thread, the memory model obeys SC. If the completion order is different from the program order, then the memory model is a variant of WC. ",
        "page_idx": 588
    },
    {
        "type": "text",
        "text": "Definition 127 ",
        "text_level": 1,
        "page_idx": 589
    },
    {
        "type": "text",
        "text": "A memory request is said to be atomic or observe atomicity, when it is perceived to execute instantaneously by all threads at some point of time after it is issued. ",
        "page_idx": 589
    },
    {
        "type": "text",
        "text": "Important Point 18 ",
        "text_level": 1,
        "page_idx": 589
    },
    {
        "type": "text",
        "text": "To be precise, for every memory request, there are three events of interest namely start, finish, and completion. Let us consider a write request. The request starts when the instruction sends the request to the L1 cache in the MA stage. The request finishes when the instruction moves to the RW stage. In modern processors, there is no guarantee that the write would have reached the target memory location when the memory request finishes. The point of time at which the write request reaches the memory location and the write is visible to all the processors, is known as the time of completion. In simple processors, the time of completion of a request is in between the start and finish times. However, in high-performance processors, this is seldom the case. This concept is shown in the following illustration. ",
        "page_idx": 589
    },
    {
        "type": "image",
        "img_path": "images/53b788db709bb846dc53c958df41181ff9b762abb135678d30c27dfdc25aaeac.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 589
    },
    {
        "type": "text",
        "text": "What about a read request? Most readers will naively assume that the completion time of a read is between the start and finish times, because it needs to return with the value of the memory location. This is however not strictly true in complex multiprocessors where reads and writes issued by the same core can be reordered if they access different memory locations. Things actually get very complicated when we consider everything that can possibly happen. We also have a bunch of memory models where writes themselves are not atomic \u2013 this means that they appear to complete at different points of time to different threads. This further complicates matters. ",
        "page_idx": 589
    },
    {
        "type": "text",
        "text": "Trivia 4 ",
        "text_level": 1,
        "page_idx": 589
    },
    {
        "type": "text",
        "text": "Here, is an incident from your author\u2019s life. He had 200 US dollars in his bank account. He had gotten a check for 300\\$ from his friend. He went to his bank\u2019s nearest ATM and deposited the check. Three days later, he decided to pay his rent (400\\$). He wrote a check to his landlord and sent it to his postal address. A day later, he got an angry phone call from his landlord informing him that his check had bounced. How was this possible? ",
        "page_idx": 589
    },
    {
        "type": "text",
        "text": "Your author then inquired. It had so happened that because of a snow storm, his bank was not able to send people to collect checks from the ATM. Hence, when his landlord deposited the check, the bank account did not have sufficient money. ",
        "page_idx": 590
    },
    {
        "type": "text",
        "text": "This example is related with the problem of memory consistency. Your author leaving his house to drop the check in the ATM is the start time. He finished the job when he dropped the check in the ATM\u2019s drop box. However, the completion time was 5 days later, when the amount was actually credited to his account. Concurrently, another thread (his landlord) deposited his rent check, and it bounced. This is an example of weak consistency in real life. ",
        "page_idx": 590
    },
    {
        "type": "text",
        "text": "There is an important point to note here. In a weak memory model, the ordering between independent memory operations in the same thread is not respected. For example, when we wrote to $x$ , and then to $y$ , thread 2 perceived them to be in the reverse order. However, the ordering of operations of memory instructions to the same address and belonging to the same thread is always respected. For example, if we set the value of a variable $x$ to $^ 1$ , and later read it in the same thread, we will either get 1 or the value written by a later write to $x$ . All the other threads will perceive the memory requests to be in the same order. Such accesses (to the same variable in the same thread) are not reordered in any memory model that your author is aware of (refer to Figure 12.7). It is possible to prove that doing so breaks the notion of a shared memory altogether (refer to[Sarangi, ]). ",
        "page_idx": 590
    },
    {
        "type": "image",
        "img_path": "images/940869fb4f5e18639ea6b22e4d7195ac69b9e6d5df7272a3fe18f8ad4cd71153.jpg",
        "img_caption": [
            "Figure 12.7: Actual completion times of memory requests in a multithreaded program "
        ],
        "img_footnote": [],
        "page_idx": 590
    },
    {
        "type": "text",
        "text": "Examples ",
        "text_level": 1,
        "page_idx": 590
    },
    {
        "type": "text",
        "text": "Let us now illustrate the difficulty with using a weak memory model that does not obey any ordering rules. Let us write our program to add numbers in parallel assuming a sequentially consistent system. Note that here we do not use OpenMP because OpenMP does a lot behind the scenes to ensure that programs run correctly on machines with weak memory models. Let us define a parallel construct that runs a block of code in parallel, and a getT hreadId() function that returns the identifier of the thread. The range of the thread ids is from 0 to $N - 1$ . The code for the parallel add function is shown in Example 153. We assume that before the parallel section begins, all the arrays are initialized to 0. In the parallel section, each thread adds its portion of numbers, and writes the result to its corresponding entry in the array, partialSums. Once, it is done, it sets its entry in the f inished array to 1. ",
        "page_idx": 590
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 591
    },
    {
        "type": "text",
        "text": "Let us now consider the thread that needs to aggregate the results. It needs to wait for all the threads to finish the job of computing the partial sums. It does this by waiting till all the entries of the f inished array are equal to 1. Once, it establishes that all the entries in the f inished array are equal to 1, it proceeds to add all the partial sums to get the final result. The reader can readily verify that if we assume a sequentially consistent system, then this piece of code executes correctly. She needs to note that we compute the result, only when we read all the entries in the array f inished to be 1. An entry in the f inished array is equal to 1 if the partial sum is computed, and written to the partialSums array. Since we add the elements of the partialSums array to compute the final result, we can conclude that it is calculated correctly. Note that this is not a formal proof (left as an exercise for the reader). ",
        "page_idx": 591
    },
    {
        "type": "text",
        "text": "Example 153 ",
        "text_level": 1,
        "page_idx": 591
    },
    {
        "type": "text",
        "text": "Write a shared memory program to add a set of numbers in parallel on a sequentially consistent machine. ",
        "page_idx": 591
    },
    {
        "type": "text",
        "text": "Answer: Let us assume that all the numbers are already stored in an array called numbers.   \nThe array numbers has SIZE entries. The number of parallel threads is given by $N$ .   \n/\\* variable declaration \\*/   \nint partialSums[N];   \nint finished[N];   \nint numbers[SIZE];   \nint result = 0;   \nint doneInit = 0;   \n/\\* initialize all the elements in partialSums and finished to 0 \\*/   \ndoneInit = 1;   \n/\\* parallel section \\*/   \nparallel { /\\* wait till initialization \\*/ while (!doneInit()){}; /\\* compute the partial sum \\*/ int myId $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ getThreadId(); int startIdx $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ myId \\* SIZE/N; int endIdx $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ startIdx $^ +$ SIZE/N; ",
        "page_idx": 591
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 591
    },
    {
        "type": "table",
        "img_path": "images/924e5143088996c0a4fc09e7013b446324e345a15d3eef1f311c0e6771137bbe.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>for(int jdx = startIdx; jdx < endIdx; jdx++) partialSums[myId] += numbers[jdx]; finished[myId] = 1;</td></tr><tr><td>/* set an entry in the finished array */</td></tr><tr><td>for (int i=O;i < N;i++){</td></tr><tr><td>} /* wait till all the threads are done */</td></tr><tr><td>do{ flag = 1;</td></tr></table></body></html>\n\n",
        "page_idx": 592
    },
    {
        "type": "text",
        "text": "Now, let us consider a weak memory model. We implicitly assumed in our example with sequential consistency that when the last thread reads finished[i] to be 1, partialSums[i] contains the value of the partial sum. However, this assumption does not hold if we assume a weak memory model because the memory system might reorder the writes to $f i n i s h e d [ i ]$ and $p a r t i a l S u m s [ i ]$ . It is thus possible that the write to the f inished array happens before the write to the partialSums array in a system with a weak memory model. In this case, the fact that finished[i] is equal to 1 does not guarantee that $p a r t i a l S u m s [ i ]$ contains the updated value. This distinction is precisely what makes sequential consistency extremely programmer friendly. ",
        "page_idx": 592
    },
    {
        "type": "text",
        "text": "Important Point 19   \nIn a weak memory model, the memory accesses issued by the same thread are always perceived to be in program order by that thread. However, the order of memory accesses can be perceived differently by other threads. ",
        "page_idx": 592
    },
    {
        "type": "text",
        "text": "Let us come back to the problem of ensuring that our example to add numbers in parallel runs correctly. We observe that the only way out of our quagmire is to have a mechanism to ensure that the write to partialSums[i] is completed before another thread reads finished[i] to be 1. We can use a generic instruction known as a fence. This instruction ensures that all the reads and writes issued before the fence complete before any read or write after the fence begins. Trivially, we can convert a weak memory model to a sequentially consistent one by inserting a fence after every instruction. However, this can induce a large overhead. It is best to introduce a minimal number of fence instructions, as and when required. Let us look at our example for adding a set of numbers in parallel for weak memory models by adding fence instructions. ",
        "page_idx": 592
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 593
    },
    {
        "type": "text",
        "text": "Example 154 ",
        "text_level": 1,
        "page_idx": 593
    },
    {
        "type": "text",
        "text": "Write a shared memory program to add a set of numbers in parallel on a machine with a weak memory model. ",
        "page_idx": 593
    },
    {
        "type": "text",
        "text": "Answer: Let us assume that all the numbers are already stored in an array called numbers.   \nThe array numbers has SIZE entries. The number of parallel threads is given by $N$ .   \n/\\* variable declaration \\*/   \nint partialSums[N];   \nint finished[N];   \nint numbers[SIZE];   \nint result = 0;   \n/\\* initialize all the elements in partialSums and finished to 0 \\*/   \n/\\* fence \\*/   \n/\\* ensures that the parallel section can read the initialized arrays \\*/   \nfence();   \n/\\* All the data is present in all the arrays at this point \\*/   \n/\\* parallel section \\*/   \nparallel { /\\* get the current thread id \\*/ int myId $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ getThreadId(); /\\* compute the partial sum \\*/ $i n t s t a r t I d x \\ = \\ m y I d \\ * \\ S I Z E / N ;$ ; int endIdx $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ startIdx $^ +$ SIZE/N; for(int jdx = startIdx; jdx < endIdx; jdx++) partialSums[myId] $+ =$ numbers[jdx]; /\\* fence \\*/ /\\* ensures that finished[i] is written after partialSums[i] \\*/ fence(); ",
        "page_idx": 593
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 593
    },
    {
        "type": "table",
        "img_path": "images/c067fe1c6bb9a10a6ca2cfe0106fd2973aafeeec9989d31ba18e52ce041241c1.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "\n\n<html><body><table><tr><td>/* set the value of done */ finished[myId] = 1; }</td></tr><tr><td></td></tr><tr><td>/* wait till all the threads are done */ do{</td></tr><tr><td>flag = 1;</td></tr><tr><td>for (int i=O\uff1bi <N\uff1bi++){ if(finished[i] == O){</td></tr><tr><td>flag = 0;</td></tr><tr><td>break; }</td></tr><tr><td>} } while (flag == O) ;</td></tr></table></body></html>\n\n",
        "page_idx": 594
    },
    {
        "type": "text",
        "text": "Example 154 shows the code for a weak memory model. The code is more or less the same as it was for the sequentially consistent memory model. The only difference is that we have added two additional fence instructions. We assume a function called $f e n c e ( )$ that internally invokes a fence instruction. We first call $f e n c e ( )$ before creating all the parallel threads. This ensures that all the writes for initializing data structures have completed. After that we start the parallel threads. The parallel threads finish the process of computing and writing the partial sum, and then we invoke the fence operation again. This ensures that before $f i n i s h e d [ m y I d ]$ is set to 1, all the partial sums have been computed and written to their respective locations in memory. Secondly, if the last thread reads $f i n i s h e d [ i ]$ to be 1, then we can say for sure that the value of partialSums[i] is up-to-date and correct. Hence, this program executes correctly, in spite of a weak memory model. ",
        "page_idx": 594
    },
    {
        "type": "text",
        "text": "We thus observe that weak memory models do not sacrifice on correctness if the programmer is aware of them, and inserts fences at the right places. Nonetheless, it is necessary for programmers to be aware of weak memory models, and they need to also understand that a lot of subtle bugs in parallel programs occur because programmers do not take the underlying memory model into account. Weak memory models are currently used by most processors because they allow us to build high-performance memory systems. In comparison, sequential consistency is very restrictive, and other than the MIPS R10000 [Yeager, 1996] no other major vendor offers machines with sequential consistency. All our current x86- and ARM-based machines use different versions of weak memory models. ",
        "page_idx": 594
    }
]