12.2.3 Amdahl’s Law

We have just taken a look at examples for adding a set of  numbers in parallel using both the paradigms namely shared memory and message passing. We divided our program into two parts – a sequential part and a parallel part (refer to Figure 12.3). In the parallel part of the execution, each thread completed the work assigned to it, and created a partial result. In the sequential part, the root or master or parent thread initialized all the variables and data structures, and spawned all the child threads. After all the child threads completed (or joined), the parent thread aggregated the results produced by all the child threads. This process of aggregating results is also known as reduction. The process of initializing variables, and reduction, are both sequential.

Let us now try to derive the speedup of a parallel program vis-a-vis its sequential counterpart. Let us consider a program that takes  units of time to execute. Let  be the fraction of time that it spends in its sequential part, and  be the fraction of time that it spends in its parallel part. The sequential part is unaffected by parallelism; however, the parallel part gets equally divided among the processors. If we consider a system of  processors, then the parallel part is expected to have a speedup of  . Thus, the time  that the parallel version of the program takes is equal to:

Alternatively, the speedup (  ) is given by:

Equation 12.2 is known as the Amdahl’s Law. It is a theoretical estimate (or rather the upper bound in most cases) of the speedup that we expect with additional parallelism.

Figure 12.4 plots the speedups as predicted by Amdahl’s Law for three values of  (10%,  , and  ). We observe that with an increasing number of processors the speedup gradually saturates and tends to the limiting value,  . We observe diminishing returns as we increase the number of processors beyond a certain point. For example, for  , there is no appreciable difference in speedups between a system with 35 processors, and a system with 200 processors. We approach similar limits for all three values of  . The important point to note here is that increasing speedups by adding additional processors has its limits. We cannot expect to keep getting speedups indefinitely by adding more processors, because we are limited by the length of the sequential sections in programs.

To summarize, we can draw two inferences. The first is that to speed up a program it is necessary to have as much parallelism as possible. Hence, we need to have a very efficient parallel programming library, and parallel hardware. However, parallelism has its limits, and it is not possible to increase the speedup appreciably beyond a certain limit. The speedup is limited by the length of the sequential section in the program. To reduce the sequential section, we need to adopt approaches both at the algorithmic level, and at the system level. We need to design our algorithms in such a way that the sequential section is as short as possible. For example, in Examples 149, and 150, we can also perform the initialization in parallel (reduces the length of the sequential section). Secondly, we need a fast processor that can minimize the time it takes to execute the sequential section.

We looked at the latter requirement (designing fast processors) in Chapters 9, 10, and 11 Now, let us look at designing fast and power efficient hardware for the parallel section.