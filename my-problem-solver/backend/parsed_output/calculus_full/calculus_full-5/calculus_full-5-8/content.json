[
    {
        "type": "text",
        "text": "5.8 Numerical Integration ",
        "text_level": 1,
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "This section concentrates on definite integrals. The inputs are $y ( x )$ and two endpoints $a$ and $b$ : The output is the integral $I$ : Our goal is to find that number $\\textstyle \\int _ { a } ^ { b } y ( x ) d x = I$ , accurately and in a short time. Normally this goal is achievable\u2014as soon as we have a good method for computing integrals. ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Our two approaches so far have weaknesses. The search for an antiderivative succeeds in important case\u0004s\u0004, and Chapter 7 extends that range\u2014but \u0004g\u0004e\u0004nerall y\u0001 $f ( x )$ is not available. The other approach (by rectangles) is in the right direction but too crude. The height is set by $y ( x )$ at the right and left\u0001end of each small interval. The right and left rectangle rules add the areas ( $\\Delta x$ times $y$ ): ",
        "page_idx": 286
    },
    {
        "type": "equation",
        "text": "$$\nR _ { n } = ( \\Delta x ) ( y _ { 1 } + y _ { 2 } + \\cdots + y _ { n } ) \\quad { \\mathrm { a n d } } \\quad L _ { n } = ( \\Delta x ) ( y _ { 0 } + y _ { 1 } + \\cdots + y _ { n - 1 } ) .\n$$",
        "text_format": "latex",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "The value of $y ( x )$ at the end of interval $j$ is $y _ { j }$ : The extreme left value $y _ { 0 } = y ( a )$ enters $L _ { n }$ : With $n$ equal intervals of length $\\Delta x = ( b - a ) / n$ , the extreme right value is $y _ { n } = y ( b )$ : It enters $R _ { n }$ : Otherwise the sums are the same\u2014simple to compute, easy to visualize, but very inaccurate. ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "This section goes from slow methods (rectangles) to better methods (trapezoidal and midpoint) to good methods (Simpson and Gauss). Each improvement cuts down the error. You could discover the formulas without the book, by integrating $x$ and $x ^ { 2 }$ and $x ^ { 4 }$ : The rule $R _ { n }$ would come out on one side of the answer, and $L _ { n }$ would be on the other side. You would figure out what to do next, to come closer to the exact integral. The book can emphasize one key point: ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "The quality of a formula depends on how many integrals $\\textstyle \\int 1 d x , \\int x d x , \\int x ^ { 2 } d x , \\dotsc$ ; it computes exactly. If $\\int x ^ { p } d x$ is the first to be wrong, the order of accuracy is $p$ : ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "By testing the integrals of $1 , x , x ^ { 2 } , \\ldots$ , we decide how accurate the formulas are. ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Figure 5.18 shows the rectangle rules $R _ { n }$ and $L _ { n }$ : They are already wrong when $y = x$ . These methods are first-order: $p = 1$ : The errors involve the first power of $\\Delta x$ \u2014where we would much prefer a higher power. A larger $p$ in $( \\Delta x ) ^ { p }$ means a smaller error. ",
        "page_idx": 286
    },
    {
        "type": "image",
        "img_path": "images/60b1e20f50aa5ec37f47b11e0a801f51bf0e5b85243a771c923bdf2d0070ac85.jpg",
        "img_caption": [
            "\u0001Fig. 5.18 Errors $E$ and $e$ in $R _ { n }$ and $L _ { n }$ are the areas of triangles. "
        ],
        "img_footnote": [],
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "When th\u0001e graph of $y ( x )$ is a s\u0004tr\u0004a\u0004ight line, the\u0001int e\u0001gral $I$ is known\u0001. The error triangles $E$ and $e$ have base $\\Delta x$ : Their heights are the differences $y _ { j + 1 } - y _ { j }$ : The areas are $\\scriptstyle { \\frac { 1 } { 2 } } ( { \\mathrm { b a s e } } ) ( { \\mathrm { h e i g h t } } )$ , and the only difference is a minus sign. ( $L$ is too low, so the error $L - I$ is negative.) The total error in $R _ { n }$ is the sum of the $E$ \u2019s: ",
        "page_idx": 286
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r } { R _ { n } - I = \\frac 1 2 { \\Delta x } ( y _ { 1 } - y _ { 0 } ) + \\cdots + \\frac 1 2 { \\Delta x } ( y _ { n } - y _ { n - 1 } ) = \\frac 1 2 { \\Delta x } ( y _ { n } - y _ { 0 } ) . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "All $y$ \u2019s between $y _ { 0 }$ and $y _ { n }$ cancel. Similarly for the sum of the $e$ \u2019s: ",
        "page_idx": 286
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r } { L _ { n } - I = - \\frac { 1 } { 2 } \\Delta x ( y _ { n } - y _ { 0 } ) = - \\frac { 1 } { 2 } \\Delta x [ y ( b ) - y ( a ) ] . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "The greater the slope of $y ( x )$ , the greater the error\u2014since rectangles have zero slope. Formulas (1) and (2) are nice\u2014but those errors are large. To integrate $y = x$ from $a = 0$ to $b = 1$ ; the error is $\\begin{array} { r } { { \\frac { 1 } { 2 } } \\Delta x ( 1 - 0 ) } \\end{array}$ : It takes 500; 000 rec\u00d1tangles to reduce this error to $1 / 1 , 0 0 0 , 0 0 0$ . This accuracy is reasonable, but that many rectangles is unacceptable. ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The beauty of the error formulas is that they are \u201casymptotically correct\u201d for all functions. W?hen the graph is curved, the errors don\u2019t fit exactly into triangles. But the ratio of predicted error to actual error approaches 1. As $\\Delta x \\to 0$ , the graph is almost straight in each interval\u2014thi\u0001s is linear approximation. ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The error prediction $\\begin{array} { r } { \\frac { 1 } { 2 } \\Delta x [ y ( b ) - y ( a ) ] } \\end{array}$ is\u0001so simp\u0001le that we\u0001test it on $y ( x ) = \\sqrt { x }$ : ",
        "page_idx": 287
    },
    {
        "type": "equation",
        "text": "$$\n{ \\begin{array} { r l r l r l r } { I = \\int _ { 0 } ^ { 1 } { \\sqrt { x } } d x = { \\frac { 2 } { 3 } } } & { n = } & { 1 } & { 1 0 } & { 1 0 0 } & { 1 0 0 0 } \\\\ & { } & { { \\mathrm { e r r o r } } R _ { n } - I = } & { . 3 3 } & { . 0 4 4 } & { . 0 0 4 8 } & { . 0 0 0 4 9 } \\\\ & { } & { { \\mathrm { e r r o r } } L _ { n } - I = } & { - . 6 7 } & { - . 0 5 6 } & { - . 0 0 5 2 } & { - . 0 0 0 5 1 } \\end{array} }\n$$",
        "text_format": "latex",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The error decreases along each row. So does $\\Delta x = . 1 , . 0 1 , . 0 0 1 , . 0 0 0 1$ : Multiplying $n$ by 10 divides $\\Delta x$ by 10: The error is also divided by 10 (almost). The error is nearly proportional to $\\Delta x$ \u2014typical of first-order methods. ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The predicted error is $\\scriptstyle { \\frac { 1 } { 2 } } \\Delta x$ , since here $y ( 1 ) = 1$ and $y ( 0 ) = 0$ : The computed errors in the table come closer and closer to $^ { \\frac { 1 } { 2 } } \\Delta x = . 5 , . 0 5 , . 0 0 5 , . 0 0 0 5$ : The prediction is the \u201cleading term\u201d in the actual error. ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The table also shows a curious fact. Subtracting the last row from the row above gives exact numbers 1; :1; :01; and .001: This is $( R _ { n } - I ) - ( L _ { n } - I )$ ; which is $R _ { n } - L _ { n }$ : It comes from an extra rectangle at the right, included in $R _ { n }$ but not $L _ { n }$ : Its height is 1 and its area is $1 , . 1 , . 0 1 , . 0 0 1$ : ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The e\u0001rrors in $R _ { n }$ and $L _ { n }$ almost ca\u0004nce\u0001l. The av\u0004era\u0001ge $\\begin{array} { r } { T _ { n } = \\frac { 1 } { 2 } ( R _ { n } + L _ { n } ) } \\end{array}$ ha\u0001s less error\u2014it is the \u201ctrapezoidal rule.\u201d First we give the rectangle prediction two final tests: ",
        "page_idx": 287
    },
    {
        "type": "equation",
        "text": "$$\n{ \\begin{array} { l l l l } & { n = 1 } & { n = 1 0 } & { n = 1 0 0 } & { n = 1 0 0 0 } \\\\ { \\int \\left( x ^ { 2 } - x \\right) d x : } & { { \\mathrm { e r r o r s } } } & { 1 . 7 \\cdot 1 0 ^ { - 1 } } & { 1 . 7 \\cdot 1 0 ^ { - 3 } } & { 1 . 7 \\cdot 1 0 ^ { - 5 } } \\\\ { \\int d x / ( 1 0 + \\cos 2 \\pi x ) : } & { { \\mathrm { e r r o r s } } } & { - 1 \\cdot 1 0 ^ { - 3 } } & { 2 \\cdot 1 0 ^ { - 1 4 } } & { \\cdot \\circ \\mathfrak { o } ^ { , } } & { \\cdot \\circ \\mathfrak { o } ^ { , } } \\end{array} }\n$$",
        "text_format": "latex",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "Those errors are falling faster than $\\Delta x$ : For $y = x ^ { 2 } - x$ the prediction explains why: $y ( 0 )$ equals $y ( 1 )$ : The leading term, with $y ( b )$ minus $y ( a )$ , is zero. The exact errors are $\\scriptstyle { \\frac { 1 } { 6 } } ( \\Delta x ) ^ { 2 }$ , dropping from $1 0 ^ { - 1 }$ to $1 0 ^ { - 3 }$ to $1 0 ^ { - 5 }$ to $1 0 ^ { - 7 }$ : In these examples $L _ { n }$ is identical to $R _ { n }$ (and also to $T _ { n }$ ), because the end rectangles are the same. We will see th1ese $\\scriptstyle { \\frac { 1 } { 6 } } ( \\Delta x ) ^ { 2 }$ errors in the trapezoidal rule. ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The last row in the table is more unusual. It shows practically no error. Why do the rectangle rules suddenly achieve such an outstanding success? ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The reason is that $y ( x ) = 1 / ( 1 0 + \\cos 2 \\pi x )$ is periodic. The leading term in the error is zero, because $y ( 0 ) = y ( 1 )$ : Also the next term will be zero, because $y ^ { \\prime } ( 0 ) = y ^ { \\prime } ( 1 )$ : Every power of $\\Delta x$ is multiplied by zero, when we integrate over a complete period. S?o the errors go to zero exponentially fast. ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "Personal note I tried to integrate $1 / ( 1 0 + \\cos 2 \\pi x )$ by hand and failed. Then I was embarrassed to discover the answer in my book on applied mathematics. The method was a special trick using complex numbers, which applies over an exact period. Finally I found the antiderivative (quite complicated) in a handbook of integrals, and verified the area $1 / { \\sqrt { 9 9 } }$ : ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "5.8 Numerical Integration ",
        "text_level": 1,
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "THE TRAPEZOIDAL AND MIDPOINT RULES",
        "text_level": 1,
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "We move to integration formulas that are exact when $y = x$ : They have second-order accuracy. The $\\Delta x$ error term disappears. The formulas give the correct area under straight lines. The predicted error is a multiple of $( \\Delta x ) ^ { 2 }$ : That multiple is found by testing $y = x ^ { 2 }$ \u2014for which the \u0004a\u0004n\u0004swers are not exact. ",
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "The first formula combines $R _ { n }$ and $L _ { n }$ : To cancel as much error as possible, take the average ${ \\frac { 1 } { 2 } } ( R _ { n } + L _ { n } )$ : This yields the trapezoidal rule, which approximates $\\int y ( x ) d x$ by $T _ { n }$ : ",
        "page_idx": 288
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r } { T _ { n } = \\frac { 1 } { 2 } R _ { n } + \\frac { 1 } { 2 } L _ { n } = \\Delta x ( \\frac { 1 } { 2 } y _ { 0 } + y _ { 1 } + \\cdots + y _ { n - 1 } + \\frac { 1 } { 2 } y _ { n } ) . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "Another way to find $T _ { n }$ is from the area of the \u201ctrapezoid\u201d below $y = x$ in Figure $5 . 1 9 \\mathrm { a }$ . ",
        "page_idx": 288
    },
    {
        "type": "image",
        "img_path": "images/f980f9acc2e6899ad80a6a88905d0aabb84005c1f0a6a7eb03728ad353b215cf.jpg",
        "img_caption": [
            "Fig. 5.19 Second-order accuracy: The error prediction is based on $v = x ^ { 2 }$ : "
        ],
        "img_footnote": [],
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "The base is $\\Delta x$ and the sides have heights $y _ { j - 1 }$ and $y _ { j }$ : Adding those areas gives ${ \\frac { 1 } { 2 } } ( L _ { n } + R _ { n } )$ in formula (3)\u2014the coefficien\u0001ts of $y _ { j }$ combine into $\\textstyle { \\frac { 1 } { 2 } } + { \\frac { 1 } { 2 } } = 1$ : Only the first and last intervals are missing a neighbor, so the rule has $\\scriptstyle { \\frac { 1 } { 2 } } y _ { 0 }$ and $\\textstyle { \\frac { 1 } { 2 } } y _ { n }$ : Because trapezoids (unlike rectangles) fit under a sloping line, $T _ { n }$ is exact when $y = x$ : ",
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "What is the difference from rectangles? The trapezoidal rule gives weight ${ \\scriptstyle { \\frac { 1 } { 2 } } } \\Delta x$ to $y _ { 0 }$ and $y _ { n }$ : The rectangle rule $R _ { n }$ gives full weight $\\Delta x$ to $y _ { n }$ (and n\u0001o weight to $y _ { 0 }$ ). $R _ { n } - T _ { n }$ is exactly the leading error $\\textstyle { \\frac { 1 } { 2 } } y _ { n } - { \\frac { 1 } { 2 } } y _ { 0 }$ : The change to $T _ { n }$ knocks out that error. ",
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "Another important formula is exact for $y ( x ) = x$ : A rectangle has the same area as a trapezoid, if the height of the rectangle is h\u0004a\u0004l\u0004fway b \u0001etween $y _ { j - 1 }$ and $y _ { j }$ : On a straight line graph that is achieved at the midpoint of the interval. By evaluating $y ( x )$ at the halfway points $\\begin{array} { r } { \\frac { 1 } { 2 } \\Delta x , \\frac { 3 } { 2 } \\Delta x , \\frac { 5 } { 2 } \\Delta x , } \\end{array}$ : : :, we get much better rectangles. This leads to the midpoint rule $M _ { n }$ : ",
        "page_idx": 288
    },
    {
        "type": "equation",
        "text": "$$\nM _ { n } = \\Delta x ( y _ { 1 / 2 } + y _ { 3 / 2 } + \\cdots + y _ { n - 1 / 2 } ) .\n$$",
        "text_format": "latex",
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "For $\\int _ { 0 } ^ { 4 } x d x$ , trapezoids give $\\textstyle { \\frac { 1 } { 2 } } ( 0 ) + 1 + 2 + 3 + { \\frac { 1 } { 2 } } ( 4 ) = 8 .$ : The midpoint rule gives $\\begin{array} { r } { \\frac { 1 } { 2 } + \\frac { 3 } { 2 } + \\frac { 5 } { 2 } + \\frac { 7 } { 2 } = 8 } \\end{array}$ , again correct.\u0001The rules become different when $y = x ^ { 2 }$ ; because $y _ { 1 / 2 }$ is no longer the average of $y _ { 0 }$ and $y _ { 1 }$ :\u0001Try both s\u0001econd-order\u0001rules on $x ^ { 2 }$ : ",
        "page_idx": 288
    },
    {
        "type": "equation",
        "text": "$$\n{ \\begin{array} { r l r l r l } { I = \\int _ { 0 } ^ { 1 } x ^ { 2 } d x } & { n = } & { 1 } & { 1 0 } & { 1 0 0 } \\\\ & { { \\mathrm { e r r o r } } T _ { n } - I = } & { 1 / 6 } & { 1 / 6 0 0 } & { 1 / 6 0 0 0 0 } \\\\ & { { \\mathrm { e r r o r } } M _ { n } - I = } & { - 1 / 1 2 } & { - 1 / 1 2 0 0 } & { - 1 / 1 2 0 0 0 0 } \\end{array} }\n$$",
        "text_format": "latex",
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "The errors fall by 100 when $n$ is multiplied by 10: The midpoint rule is twice as good $( - 1 / 1 2$ vs. $1 / 6 \\AA$ ). Since all smooth functions are close to parabolas (quadratic ",
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "approximation in each interval), the leading errors come from Figure 5.19. The trapezoidal error is exactly $\\scriptstyle { \\frac { 1 } { 6 } } ( \\Delta x ) ^ { 2 }$ when $y ( x )$ is $x ^ { 2 }$ (the 12 in the formula divides the 2 i\u0001n $y ^ { \\prime }$ )\u0013: ",
        "page_idx": 289
    },
    {
        "type": "equation",
        "text": "$$\nT _ { n } - I \\approx { \\frac { 1 } { 1 2 } } ( \\Delta x ) ^ { 2 } \\left[ ( y _ { 1 } ^ { \\prime } - y _ { 0 } ^ { \\prime } ) + \\cdots + ( y _ { n } ^ { \\prime } - y _ { n - 1 } ^ { \\prime } ) \\right] = { \\frac { 1 } { 1 2 } } ( \\Delta x ) ^ { 2 } \\left[ y _ { n } ^ { \\prime } - y _ { 0 } ^ { \\prime } \\right]\n$$",
        "text_format": "latex",
        "page_idx": 289
    },
    {
        "type": "equation",
        "text": "$$\nM _ { n } - I \\approx - \\frac { 1 } { 2 4 } ( \\Delta x ) ^ { 2 } \\left[ y _ { n } ^ { \\prime } - y _ { 0 } ^ { \\prime } \\right] = - \\frac { 1 } { 2 4 } ( \\Delta x ) ^ { 2 } \\left[ y ^ { \\prime } ( b ) - y ^ { \\prime } ( a ) \\right]\n$$",
        "text_format": "latex",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "For exact error formulas, change $y ^ { \\prime } ( b ) - y ^ { \\prime } ( a )$ to $( b - a ) y ^ { \\prime \\prime } ( c )$ : The location of $c$ is unknown (as in the Mean Value Theorem). In practice these formulas are not much used\u2014they involve the $p$ th derivative at an unknown location $c$ : The main point about the error is the factor $( \\Delta x ) ^ { p }$ \u0001: ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "One crucial fact is easy to overlook in our tests. Each value of $y ( x )$ can be extremely hard to compute. Every time a formula asks for $y _ { j }$ , a computer calls a subroutine. The goal of numerical integration is to get below the error tolerance, while calling for a minimum number of values of $y$ . Second-order rules\u0001need about a thousand values for a typical tolerance of $1 0 ^ { - 6 }$ : The next methods are better. ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "FOURTH-ORDER RULE: SIMPSON ",
        "text_level": 1,
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "The trapezoidal error is nearly twice the midpoint error ( $1 / 6$ vs. $- 1 / 1 2 \\rangle$ . So a good combination will have twice as much of $M _ { n }$ as $T _ { n }$ : That is Simpson\u2019s rule: ",
        "page_idx": 289
    },
    {
        "type": "equation",
        "text": "$$\nS _ { n } = \\frac { 1 } { 3 } T _ { n } + \\frac { 2 } { 3 } M _ { n } = \\frac { 1 } { 6 } \\Delta x \\left[ y _ { 0 } + 4 y _ { 1 / 2 } + 2 y _ { 1 } + 4 y _ { 3 / 2 } + 2 y _ { 2 } + \\cdots + 4 y _ { n - 1 / 2 } + y _ { n } \\right] .\n$$",
        "text_format": "latex",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "Multiply the midpoint values by $2 / 3 = 4 / 6$ : The endpoint values are multiplied by $2 / 6$ ; except at the far ends $a$ and $b$ (with heights $y _ { 0 }$ and $y _ { n }$ ). This $1 - 4 - 2 - 4 -$ $2 - 4 - 1$ pattern has become famous. ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "Simpson\u2019s rule goes deeper than a combination of $T$ and $M$ : It comes from a parabolic approximation to $y ( x )$ in each interval. When a parabola goes through $y _ { 0 }$ ; $y _ { 1 / 2 } , y _ { 1 }$ , the area under it is $\\begin{array} { r } { \\frac { 1 } { 6 } \\Delta x ( y _ { 0 } + 4 y _ { 1 / 2 } + y _ { 1 } ) } \\end{array}$ . This is $S$ over the first interval. All our rules are constructed this way: Integrate correctly as many powers $1 , x , x ^ { 2 }$ ; : : : as possible. Parabolas are better than straight lines, which are better than flat pieces. $S$ beats $M$ , which beats $R$ : Check Simpson\u2019s rule on powers of $x$ , with $\\Delta x = 1 / n$ : ",
        "page_idx": 289
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { c c c c } { { } } & { { n = 1 } } & { { n = 1 0 } } & { { n = 1 0 0 } } \\\\ { { \\mathrm { e r r o r i f } y = x ^ { 2 } } } & { { 0 } } & { { 0 } } & { { 0 } } \\\\ { { \\mathrm { e r r o r i f } y = x ^ { 3 } } } & { { 0 } } & { { 0 } } & { { 0 } } \\\\ { { \\mathrm { e r r o r i f } y = x ^ { 4 } } } & { { 8 . 3 3 \\cdot 1 0 ^ { - 3 } } } & { { 8 . 3 3 \\cdot 1 0 ^ { - 7 } } } & { { 8 . 3 3 \\cdot 1 0 ^ { - 1 1 } } } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "Exact ans\u00bbwers for $x ^ { 2 }$ are no surprise. $S _ { n }$ was selected to get parabolas right. But the zero errors for $x ^ { 3 }$ were not expected. The accuracy has jumped to fourth order, with errors prop\u0001ortional to $( \\Delta x ) ^ { 4 }$ :\u0001That explains the popularity of Simpson\u2019s rule. ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "To understand why $x ^ { 3 }$ is integrated exactly, look at the interval $[ - 1 , 1 ]$ : The odd function $x ^ { 3 }$ has zero integral, and Simpson agrees by symmetry: ",
        "page_idx": 289
    },
    {
        "type": "equation",
        "text": "$$\n\\int _ { - 1 } ^ { 1 } x ^ { 3 } d x = { \\frac { 1 } { 4 } } x ^ { 4 } { \\Bigg ] } _ { - 1 } ^ { 1 } = 0 \\quad { \\mathrm { a n d } } \\quad { \\frac { 2 } { 6 } } { \\Big [ } ( - 1 ) ^ { 3 } + 4 ( 0 ) ^ { 3 } + 1 ^ { 3 } { \\Big ] } = 0 .\n$$",
        "text_format": "latex",
        "page_idx": 289
    },
    {
        "type": "image",
        "img_path": "images/b218ee2c2a5bb31eedefb4227723579f467b601d91f7c3484e8808a7cb506dcd.jpg",
        "img_caption": [
            "Fig. 5.20 Simpson versus Gauss: $E = c ( \\Delta x ) ^ { 4 } ( y _ { j + 1 } ^ { \\prime \\prime \\prime } - y _ { j } ^ { \\prime \\prime \\prime } )$ with $c _ { S } = 1 / 2 8 8 0$ and $c _ { G } = - 1 / 4 3 2 0$ : "
        ],
        "img_footnote": [],
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "THE GAUSS RULE?(OPTIONAL?)",
        "text_level": 1,
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "We need a competitor for Simpson, and Gauss can compete with anybody. He calculated integrals in astro\u0001nomy, and discovered that two points are enough for $a$ fourth-order method. From $^ { - 1 }$ to 1 (a single interval) his rule is ",
        "page_idx": 290
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r } { \\int _ { - 1 } ^ { 1 } y ( x ) d x \\approx y ( - 1 / \\sqrt { 3 } ) + y ( 1 / \\sqrt { 3 } ) . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "Those \u201cGauss point s\u0001\u201d $x = - 1 / \\sqrt { 3 }$ and $x = 1 / \\sqrt { 3 }$ can be found\bdi?rectly. By placing them symmetrically, all odd powers $x , x ^ { 3 }$ ; : : : are correctly integrated. The key is in $y = x ^ { 2 }$ ,?whose integral is $2 / 3$ : The Gauss points $- x _ { G }$ and $+ x _ { G }$ get this integral right: ",
        "page_idx": 290
    },
    {
        "type": "equation",
        "text": "$$\n{ \\frac { 2 } { 3 } } = ( - x _ { G } ) ^ { 2 } + ( x _ { G } ) ^ { 2 } , \\mathrm { s o } x _ { G } ^ { 2 } = { \\frac { 1 } { 3 } } \\quad \\mathrm { a n d } x _ { G } = \\pm { \\frac { 1 } { \\sqrt { 3 } } } .\n$$",
        "text_format": "latex",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "Figure $5 . 2 0 \\mathrm { c }$ shifts to the interval from 0 to $\\Delta x$ : The Gauss points are $( 1 \\pm \\mathsf { \\bar { 1 } } / \\sqrt { 3 } ) \\Delta x / 2$ : They are not as convenient as S\u0004imp\u0001son\u2019s (which \u0004hand\u0001 calculators prefer). Gauss is good for thousands of integrations o\u0001ver one interval. Si\u0001mpson is good when intervals go back to back\u2014then Simpson also uses two $y$ \u2019s per interval. For $y = x ^ { 4 }$ , you see both errors drop by $1 0 ^ { - 4 }$ in comparing $n = 1$ to $n = 1 0$ : ",
        "page_idx": 290
    },
    {
        "type": "equation",
        "text": "$$\n{ \\begin{array} { r l r l r } { I = \\int _ { 0 } ^ { 1 } x ^ { 4 } d x } & { } & { { \\mathrm { S i m p s o n e r r o r } } } & { } & { 8 . 3 3 \\cdot 1 0 ^ { - 3 } } & { } & { 8 . 3 3 \\cdot 1 0 ^ { - 7 } } \\\\ & { } & { { \\mathrm { G a u s s ~ e r r o r } } } & { } & { - 5 . 5 6 \\cdot 1 0 ^ { - 3 } } & { } & { - 5 . 5 6 \\cdot 1 0 ^ { - 7 } } \\end{array} }\n$$",
        "text_format": "latex",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "DEFINITE INTEGRALS ON A CALCULATOR ",
        "text_level": 1,
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "It is fascinating to know how numerical int\u0001egration is actually\u0001done. The points are not equally spaced! For an integral from 0 to 1, Hewlett-Packard machines might internally replace $x$ by $3 u ^ { 2 } - 2 u ^ { 3 }$ (the limits on $u$ are also 0 and 1). The machine remembers to change $d x$ : For example, ",
        "page_idx": 290
    },
    {
        "type": "equation",
        "text": "$$\n\\int _ { 0 } ^ { 1 } { \\frac { d x } { \\sqrt { x } } } \\quad { \\mathrm { b e c o m e s } } \\quad \\int _ { 0 } ^ { 1 } { \\frac { 6 ( u - u ^ { 2 } ) d u } { \\sqrt { 3 u ^ { 2 } - 2 u ^ { 3 } } } } = \\int _ { 0 } ^ { 1 } { \\frac { 6 ( 1 - u ) d u } { \\sqrt { 3 - 2 u } } } .\n$$",
        "text_format": "latex",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "Algebraically that looks worse\u2014but the infinite val\u0001ue of $1 / { \\sqrt { x } }$ at $x = 0$ disappears at $u = 0$ : The differential $6 ( u - u ^ { 2 } ) d u$ was chosen to vanish at $u = 0$ and $u = 1$ : We don\u2019t need $y ( x )$ at the endpoints\u2014where infinity is most common. In the $u$ variable the integration points are equally spaced\u2014therefore in $x$ they are not. ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "When a difficult point is inside $[ a , b ]$ , break the interval in two pieces. And chop off integrals that go out to infinity. The integral of $e ^ { - x ^ { 2 } }$ should be stopped by $x = 1 0$ , since the tail is so thin. (It is bad to go too far.) Rapid oscillations are among the toughest\u2014the answer depends on cancellation of highs and lows, and the calculator requires many integration points. ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "The change from $x$ to $u$ affects periodic functions. I thought equal spacing was good, since $1 / ( 1 0 + \\cos 2 \\pi x )$ was integrated above to enormous accuracy. But there is a danger called aliasing. If sin $8 \\pi x$ is sampled with $\\Delta x = 1 / 8$ , it is always zero. A high frequency 8 is confused with a low frequency 0 (its \u201calias\u201d which agrees at the sample points). With unequal spacing the problem disappears. Notice how any integration method can be deceived: ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "Ask for the integral of $y = 0$ and specify the accuracy. The calculator samples $y$ at $x _ { 1 } , . . . , x _ { k }$ : (With a PAUSE key, the $x$ \u2019s may be displayed.) Then integrate $Y ( x ) = ( x - x _ { 1 } ) ^ { 2 } \\cdots ( x - x _ { k } ) ^ { 2 }$ : That also returns the answer zero (now wrong), because the calculator follows the same steps. ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "On the calculator you enter the function\u0001, the endpoin\u0007ts, and the accuracy. The variable $x$ can be named or not (se\u0007e the margin). The ou\u0007tp\u00a4uts 4:67077 and 4.7E-5 are the requested integral $\\displaystyle \\int _ { 1 } ^ { 2 } e ^ { x } d x$ and the estimated error bound. Your input accuracy .00001 guarantees ",
        "page_idx": 291
    },
    {
        "type": "equation",
        "text": "$$\n\\mathrm { r e l a t i v e \\ e r r o r \\ i n \\ } y = \\left| { \\frac { \\mathrm { t r u e \\ } y - \\mathrm { c o m p u t e d \\ } y } { \\mathrm { c o m p u t e d \\ } y } } \\right| \\leqslant . 0 0 0 0 1 .\n$$",
        "text_format": "latex",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "The machine estimates accuracy based on its experience in sampling $y ( x )$ : If you guarantee $e ^ { x }$ within .00000000001; it thinks you want high accuracy and takes longer. In consulting for HP, William Kahan chose formulas using $1 , 3 , 7 , 1 5 , \\dots$ sample points. Each new formula uses the samples in the previous formula. The calculator stops when answers are close. ",
        "page_idx": 291
    }
]